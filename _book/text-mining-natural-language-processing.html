<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 17 Text mining &amp; natural language processing | DataManagement.knit</title>
  <meta name="description" content="" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 17 Text mining &amp; natural language processing | DataManagement.knit" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 17 Text mining &amp; natural language processing | DataManagement.knit" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="data-visualization-1.html"/>
<link rel="next" href="cluster-computing.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="data-management-databases-and-organizations.html"><a href="data-management-databases-and-organizations.html"><i class="fa fa-check"></i>Data Management: Databases and Organizations</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#supplements"><i class="fa fa-check"></i>Supplements</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="section-1-the-managerial-perspective.html"><a href="section-1-the-managerial-perspective.html"><i class="fa fa-check"></i>Section 1 The Managerial Perspective</a></li>
<li class="chapter" data-level="1" data-path="managing-data.html"><a href="managing-data.html"><i class="fa fa-check"></i><b>1</b> Managing Data</a>
<ul>
<li class="chapter" data-level="" data-path="managing-data.html"><a href="managing-data.html#learning-objectives"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="managing-data.html"><a href="managing-data.html#introduction"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="managing-data.html"><a href="managing-data.html#individual-data-management"><i class="fa fa-check"></i>Individual data management</a></li>
<li class="chapter" data-level="" data-path="managing-data.html"><a href="managing-data.html#organizational-data-management"><i class="fa fa-check"></i>Organizational data management</a></li>
<li class="chapter" data-level="" data-path="managing-data.html"><a href="managing-data.html#problems-with-data-management-systems"><i class="fa fa-check"></i>Problems with data management systems</a></li>
<li class="chapter" data-level="" data-path="managing-data.html"><a href="managing-data.html#a-brief-history-of-data-management-systems"><i class="fa fa-check"></i>A brief history of data management systems</a></li>
<li class="chapter" data-level="" data-path="managing-data.html"><a href="managing-data.html#data-information-and-knowledge"><i class="fa fa-check"></i>Data, information, and knowledge</a></li>
<li class="chapter" data-level="" data-path="managing-data.html"><a href="managing-data.html#the-challenge"><i class="fa fa-check"></i>The challenge</a></li>
<li class="chapter" data-level="" data-path="managing-data.html"><a href="managing-data.html#summary"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="information.html"><a href="information.html"><i class="fa fa-check"></i><b>2</b> Information</a>
<ul>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#learning-objectives-1"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#introduction-1"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#a-historical-perspective"><i class="fa fa-check"></i>A historical perspective</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#a-brief-history-of-information-systems"><i class="fa fa-check"></i>A brief history of information systems</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#information-characteristics"><i class="fa fa-check"></i>Information characteristics</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#information-hardness"><i class="fa fa-check"></i>Information hardness</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#information-richness"><i class="fa fa-check"></i>Information richness</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#information-classes"><i class="fa fa-check"></i>Information classes</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#information-and-organizational-change"><i class="fa fa-check"></i>Information and organizational change</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#goal-setting-information"><i class="fa fa-check"></i>Goal-setting information</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#gap-information"><i class="fa fa-check"></i>Gap information</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#change-information"><i class="fa fa-check"></i>Change information</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#problem-solution"><i class="fa fa-check"></i>Problem solution</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#information-as-a-means-of-change"><i class="fa fa-check"></i>Information as a means of change</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#marketing"><i class="fa fa-check"></i>Marketing</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#customer-service"><i class="fa fa-check"></i>Customer Service</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#empowerment"><i class="fa fa-check"></i>Empowerment</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#information-and-managerial-work"><i class="fa fa-check"></i>Information and managerial work</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#managers-information-requirements"><i class="fa fa-check"></i>Managers’ information requirements</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#managers-needs-for-information-vary-accordingly-with-responsibilities"><i class="fa fa-check"></i>Managers’ needs for information vary accordingly with responsibilities</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#information-satisficing"><i class="fa fa-check"></i>Information satisficing</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#information-delivery-systems"><i class="fa fa-check"></i>Information delivery systems</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#information-integration"><i class="fa fa-check"></i>Information integration</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#knowledge-1"><i class="fa fa-check"></i>Knowledge</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#summary-1"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#references-and-additional-readings-1"><i class="fa fa-check"></i>References and additional readings</a></li>
<li class="chapter" data-level="" data-path="information.html"><a href="information.html#exercises-1"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="section-2-data-modeling-and-sql.html"><a href="section-2-data-modeling-and-sql.html"><i class="fa fa-check"></i>Section 2 Data Modeling and SQL</a></li>
<li class="chapter" data-level="3" data-path="the-single-entity.html"><a href="the-single-entity.html"><i class="fa fa-check"></i><b>3</b> The Single Entity</a>
<ul>
<li class="chapter" data-level="" data-path="the-single-entity.html"><a href="the-single-entity.html#the-relational-model"><i class="fa fa-check"></i>The relational model</a></li>
<li class="chapter" data-level="" data-path="the-single-entity.html"><a href="the-single-entity.html#getting-started"><i class="fa fa-check"></i>Getting started</a></li>
<li class="chapter" data-level="" data-path="the-single-entity.html"><a href="the-single-entity.html#modeling-a-single-entity-database"><i class="fa fa-check"></i>Modeling a single-entity database</a></li>
<li class="chapter" data-level="" data-path="the-single-entity.html"><a href="the-single-entity.html#creating-a-single-table-database"><i class="fa fa-check"></i>Creating a single-table database</a></li>
<li class="chapter" data-level="" data-path="the-single-entity.html"><a href="the-single-entity.html#summary-2"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="the-single-entity.html"><a href="the-single-entity.html#key-terms-and-concepts"><i class="fa fa-check"></i>Key terms and concepts</a></li>
<li class="chapter" data-level="" data-path="the-single-entity.html"><a href="the-single-entity.html#exercises-2"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-one-to-many-relationship.html"><a href="the-one-to-many-relationship.html"><i class="fa fa-check"></i><b>4</b> The One-to-Many Relationship</a>
<ul>
<li class="chapter" data-level="" data-path="the-one-to-many-relationship.html"><a href="the-one-to-many-relationship.html#relationships"><i class="fa fa-check"></i>Relationships</a></li>
<li class="chapter" data-level="" data-path="the-one-to-many-relationship.html"><a href="the-one-to-many-relationship.html#creating-a-database-with-a-1m-relationship"><i class="fa fa-check"></i>Creating a database with a 1:m relationship</a></li>
<li class="chapter" data-level="" data-path="the-one-to-many-relationship.html"><a href="the-one-to-many-relationship.html#querying-a-two-table-database"><i class="fa fa-check"></i>Querying a two-table database</a></li>
<li class="chapter" data-level="" data-path="the-one-to-many-relationship.html"><a href="the-one-to-many-relationship.html#summary-3"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="the-one-to-many-relationship.html"><a href="the-one-to-many-relationship.html#key-terms-and-concepts-1"><i class="fa fa-check"></i>Key terms and concepts</a></li>
<li class="chapter" data-level="" data-path="the-one-to-many-relationship.html"><a href="the-one-to-many-relationship.html#exercises-3"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-many-to-many-relationship.html"><a href="the-many-to-many-relationship.html"><i class="fa fa-check"></i><b>5</b> The Many-to-Many Relationship</a>
<ul>
<li class="chapter" data-level="" data-path="the-many-to-many-relationship.html"><a href="the-many-to-many-relationship.html#the-many-to-many-relationship-1"><i class="fa fa-check"></i>The many-to-many relationship</a></li>
<li class="chapter" data-level="" data-path="the-many-to-many-relationship.html"><a href="the-many-to-many-relationship.html#creating-a-relational-database-with-an-mm-relationship"><i class="fa fa-check"></i>Creating a relational database with an m:m relationship</a></li>
<li class="chapter" data-level="" data-path="the-many-to-many-relationship.html"><a href="the-many-to-many-relationship.html#querying-an-mm-relationship"><i class="fa fa-check"></i>Querying an m:m relationship</a></li>
<li class="chapter" data-level="" data-path="the-many-to-many-relationship.html"><a href="the-many-to-many-relationship.html#summary-4"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="the-many-to-many-relationship.html"><a href="the-many-to-many-relationship.html#key-terms-and-concepts-2"><i class="fa fa-check"></i>Key terms and concepts</a></li>
<li class="chapter" data-level="" data-path="the-many-to-many-relationship.html"><a href="the-many-to-many-relationship.html#exercises-4"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="one-to-one-and-recursive-relationships.html"><a href="one-to-one-and-recursive-relationships.html"><i class="fa fa-check"></i><b>6</b> One-to-One and Recursive Relationships</a>
<ul>
<li class="chapter" data-level="" data-path="one-to-one-and-recursive-relationships.html"><a href="one-to-one-and-recursive-relationships.html#learning-objectives-5"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="one-to-one-and-recursive-relationships.html"><a href="one-to-one-and-recursive-relationships.html#modeling-a-one-to-one-relationship"><i class="fa fa-check"></i>Modeling a one-to-one relationship</a></li>
<li class="chapter" data-level="" data-path="one-to-one-and-recursive-relationships.html"><a href="one-to-one-and-recursive-relationships.html#mapping-a-one-to-one-relationship"><i class="fa fa-check"></i>Mapping a one-to-one relationship</a></li>
<li class="chapter" data-level="" data-path="one-to-one-and-recursive-relationships.html"><a href="one-to-one-and-recursive-relationships.html#mapping-a-recursive-one-to-many-relationship"><i class="fa fa-check"></i>Mapping a recursive one-to-many relationship</a></li>
<li class="chapter" data-level="" data-path="one-to-one-and-recursive-relationships.html"><a href="one-to-one-and-recursive-relationships.html#querying-a-one-to-one-relationship"><i class="fa fa-check"></i>Querying a one-to-one relationship</a></li>
<li class="chapter" data-level="" data-path="one-to-one-and-recursive-relationships.html"><a href="one-to-one-and-recursive-relationships.html#querying-a-recursive-1m-relationship"><i class="fa fa-check"></i>Querying a recursive 1:m relationship</a></li>
<li class="chapter" data-level="" data-path="one-to-one-and-recursive-relationships.html"><a href="one-to-one-and-recursive-relationships.html#modeling-a-recursive-one-to-one-relationship"><i class="fa fa-check"></i>Modeling a recursive one-to-one relationship</a></li>
<li class="chapter" data-level="" data-path="one-to-one-and-recursive-relationships.html"><a href="one-to-one-and-recursive-relationships.html#mapping-a-recursive-one-to-one-relationship"><i class="fa fa-check"></i>Mapping a recursive one-to-one relationship</a></li>
<li class="chapter" data-level="" data-path="one-to-one-and-recursive-relationships.html"><a href="one-to-one-and-recursive-relationships.html#querying-a-recursive-one-to-one-relationship"><i class="fa fa-check"></i>Querying a recursive one-to-one relationship</a></li>
<li class="chapter" data-level="" data-path="one-to-one-and-recursive-relationships.html"><a href="one-to-one-and-recursive-relationships.html#modeling-a-recursive-many-to-many-relationship"><i class="fa fa-check"></i>Modeling a recursive many-to-many relationship</a></li>
<li class="chapter" data-level="" data-path="one-to-one-and-recursive-relationships.html"><a href="one-to-one-and-recursive-relationships.html#mapping-a-recursive-many-to-many-relationship"><i class="fa fa-check"></i>Mapping a recursive many-to-many relationship</a></li>
<li class="chapter" data-level="" data-path="one-to-one-and-recursive-relationships.html"><a href="one-to-one-and-recursive-relationships.html#summary-5"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="one-to-one-and-recursive-relationships.html"><a href="one-to-one-and-recursive-relationships.html#key-terms-and-concepts-3"><i class="fa fa-check"></i>Key terms and concepts</a></li>
<li class="chapter" data-level="" data-path="one-to-one-and-recursive-relationships.html"><a href="one-to-one-and-recursive-relationships.html#exercise"><i class="fa fa-check"></i>Exercise</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data-modeling.html"><a href="data-modeling.html"><i class="fa fa-check"></i><b>7</b> Data Modeling</a>
<ul>
<li class="chapter" data-level="" data-path="data-modeling.html"><a href="data-modeling.html#modeling"><i class="fa fa-check"></i>Modeling</a></li>
<li class="chapter" data-level="" data-path="data-modeling.html"><a href="data-modeling.html#data-modeling-1"><i class="fa fa-check"></i>Data modeling</a></li>
<li class="chapter" data-level="" data-path="data-modeling.html"><a href="data-modeling.html#the-building-blocks"><i class="fa fa-check"></i>The building blocks</a></li>
<li class="chapter" data-level="" data-path="data-modeling.html"><a href="data-modeling.html#data-model-quality"><i class="fa fa-check"></i>Data model quality</a></li>
<li class="chapter" data-level="" data-path="data-modeling.html"><a href="data-modeling.html#quality-improvement"><i class="fa fa-check"></i>Quality improvement</a></li>
<li class="chapter" data-level="" data-path="data-modeling.html"><a href="data-modeling.html#meaningful-identifiers"><i class="fa fa-check"></i>Meaningful identifiers</a></li>
<li class="chapter" data-level="" data-path="data-modeling.html"><a href="data-modeling.html#the-seven-habits-of-highly-effective-data-modelers"><i class="fa fa-check"></i>The seven habits of highly effective data modelers</a></li>
<li class="chapter" data-level="" data-path="data-modeling.html"><a href="data-modeling.html#summary-7"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="data-modeling.html"><a href="data-modeling.html#key-terms-and-concepts-4"><i class="fa fa-check"></i>Key terms and concepts</a></li>
<li class="chapter" data-level="" data-path="data-modeling.html"><a href="data-modeling.html#references-and-additional-readings-2"><i class="fa fa-check"></i>References and additional readings</a></li>
<li class="chapter" data-level="" data-path="data-modeling.html"><a href="data-modeling.html#exercises-5"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="reference-1-basic-structures.html"><a href="reference-1-basic-structures.html"><i class="fa fa-check"></i>Reference 1: Basic Structures</a>
<ul>
<li class="chapter" data-level="" data-path="reference-1-basic-structures.html"><a href="reference-1-basic-structures.html#one-entity"><i class="fa fa-check"></i>One entity</a></li>
<li class="chapter" data-level="" data-path="reference-1-basic-structures.html"><a href="reference-1-basic-structures.html#two-entities"><i class="fa fa-check"></i>Two entities</a></li>
<li class="chapter" data-level="" data-path="reference-1-basic-structures.html"><a href="reference-1-basic-structures.html#another-entitys-identifier-as-part-of-the-identifier"><i class="fa fa-check"></i>Another entity’s identifier as part of the identifier</a></li>
<li class="chapter" data-level="" data-path="reference-1-basic-structures.html"><a href="reference-1-basic-structures.html#exercises-6"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="normalization-and-other-data-modeling-methods.html"><a href="normalization-and-other-data-modeling-methods.html"><i class="fa fa-check"></i><b>8</b> Normalization and Other Data Modeling Methods</a>
<ul>
<li class="chapter" data-level="" data-path="normalization-and-other-data-modeling-methods.html"><a href="normalization-and-other-data-modeling-methods.html#normalization"><i class="fa fa-check"></i>Normalization</a></li>
<li class="chapter" data-level="" data-path="normalization-and-other-data-modeling-methods.html"><a href="normalization-and-other-data-modeling-methods.html#other-data-modeling-methods"><i class="fa fa-check"></i>Other data modeling methods</a></li>
<li class="chapter" data-level="" data-path="normalization-and-other-data-modeling-methods.html"><a href="normalization-and-other-data-modeling-methods.html#summary-8"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="normalization-and-other-data-modeling-methods.html"><a href="normalization-and-other-data-modeling-methods.html#key-terms-and-concepts-5"><i class="fa fa-check"></i>Key terms and concepts</a></li>
<li class="chapter" data-level="" data-path="normalization-and-other-data-modeling-methods.html"><a href="normalization-and-other-data-modeling-methods.html#references-and-additional-readings-3"><i class="fa fa-check"></i>References and additional readings</a></li>
<li class="chapter" data-level="" data-path="normalization-and-other-data-modeling-methods.html"><a href="normalization-and-other-data-modeling-methods.html#exercises-7"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="the-relational-model-and-relational-algebra.html"><a href="the-relational-model-and-relational-algebra.html"><i class="fa fa-check"></i><b>9</b> The Relational Model and Relational Algebra</a>
<ul>
<li class="chapter" data-level="" data-path="the-relational-model-and-relational-algebra.html"><a href="the-relational-model-and-relational-algebra.html#data-structures"><i class="fa fa-check"></i>Data structures</a></li>
<li class="chapter" data-level="" data-path="the-relational-model-and-relational-algebra.html"><a href="the-relational-model-and-relational-algebra.html#integrity-rules"><i class="fa fa-check"></i>Integrity rules</a></li>
<li class="chapter" data-level="" data-path="the-relational-model-and-relational-algebra.html"><a href="the-relational-model-and-relational-algebra.html#manipulation-languages"><i class="fa fa-check"></i>Manipulation languages</a></li>
<li class="chapter" data-level="" data-path="the-relational-model-and-relational-algebra.html"><a href="the-relational-model-and-relational-algebra.html#a-primitive-set-of-relational-operators"><i class="fa fa-check"></i>A primitive set of relational operators</a></li>
<li class="chapter" data-level="" data-path="the-relational-model-and-relational-algebra.html"><a href="the-relational-model-and-relational-algebra.html#a-fully-relational-database"><i class="fa fa-check"></i>A fully relational database</a></li>
<li class="chapter" data-level="" data-path="the-relational-model-and-relational-algebra.html"><a href="the-relational-model-and-relational-algebra.html#summary-9"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="the-relational-model-and-relational-algebra.html"><a href="the-relational-model-and-relational-algebra.html#key-terms-and-concepts-6"><i class="fa fa-check"></i>Key terms and concepts</a></li>
<li class="chapter" data-level="" data-path="the-relational-model-and-relational-algebra.html"><a href="the-relational-model-and-relational-algebra.html#references-and-additional-readings-4"><i class="fa fa-check"></i>References and additional readings</a></li>
<li class="chapter" data-level="" data-path="the-relational-model-and-relational-algebra.html"><a href="the-relational-model-and-relational-algebra.html#exercises-8"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="sql.html"><a href="sql.html"><i class="fa fa-check"></i><b>10</b> SQL</a>
<ul>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#introduction-3"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#data-definition"><i class="fa fa-check"></i>Data definition</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#keys"><i class="fa fa-check"></i>Keys</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#indexes"><i class="fa fa-check"></i>Indexes</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#notation"><i class="fa fa-check"></i>Notation</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#creating-a-table"><i class="fa fa-check"></i>Creating a table</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#constraints"><i class="fa fa-check"></i>Constraints</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#data-types"><i class="fa fa-check"></i>Data types</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#collation-sequence"><i class="fa fa-check"></i>Collation sequence</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#scalar-functions"><i class="fa fa-check"></i>Scalar functions</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#formatting"><i class="fa fa-check"></i>Formatting</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#altering-a-table"><i class="fa fa-check"></i>Altering a table</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#dropping-a-table"><i class="fa fa-check"></i>Dropping a table</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#creating-a-view"><i class="fa fa-check"></i>Creating a view</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#dropping-a-view"><i class="fa fa-check"></i>Dropping a view</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#creating-an-index"><i class="fa fa-check"></i>Creating an index</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#dropping-an-index"><i class="fa fa-check"></i>Dropping an index</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#data-manipulation"><i class="fa fa-check"></i>Data manipulation</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#qualifying-column-names"><i class="fa fa-check"></i>Qualifying column names</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#temporary-names"><i class="fa fa-check"></i>Temporary names</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#select"><i class="fa fa-check"></i>SELECT</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#insert"><i class="fa fa-check"></i>INSERT</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#update-1"><i class="fa fa-check"></i>UPDATE</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#delete-1"><i class="fa fa-check"></i>DELETE</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#sql-routines"><i class="fa fa-check"></i>SQL routines</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#sql-function"><i class="fa fa-check"></i>SQL function</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#sql-procedure"><i class="fa fa-check"></i>SQL procedure</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#trigger"><i class="fa fa-check"></i>Trigger</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#nullsmuch-ado-about-missing-information"><i class="fa fa-check"></i>Nulls—much ado about missing information</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#security"><i class="fa fa-check"></i>Security</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#grant"><i class="fa fa-check"></i>GRANT</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#revoke"><i class="fa fa-check"></i>REVOKE</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#the-system-catalog"><i class="fa fa-check"></i>The system catalog</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#natural-language-processing"><i class="fa fa-check"></i>Natural language processing</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#connectivity-and-odbc"><i class="fa fa-check"></i>Connectivity and ODBC</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#open-database-connectivity-odbc"><i class="fa fa-check"></i>Open database connectivity (ODBC)</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#embedded-sql"><i class="fa fa-check"></i>Embedded SQL</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#user-defined-types"><i class="fa fa-check"></i>User-defined types</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#the-future-of-sql"><i class="fa fa-check"></i>The future of SQL</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#summary-10"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#key-terms-and-concepts-7"><i class="fa fa-check"></i>Key terms and concepts</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#references-and-additional-readings-5"><i class="fa fa-check"></i>References and additional readings</a></li>
<li class="chapter" data-level="" data-path="sql.html"><a href="sql.html#exercises-9"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="reference-2-sql-playbook.html"><a href="reference-2-sql-playbook.html"><i class="fa fa-check"></i>Reference 2: SQL Playbook</a>
<ul>
<li class="chapter" data-level="" data-path="reference-2-sql-playbook.html"><a href="reference-2-sql-playbook.html#the-power-of-sql"><i class="fa fa-check"></i>The power of SQL</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="section-3-advanced-data-management.html"><a href="section-3-advanced-data-management.html"><i class="fa fa-check"></i>Section 3 Advanced Data Management</a></li>
<li class="chapter" data-level="11" data-path="spatial-and-temporal-data-management.html"><a href="spatial-and-temporal-data-management.html"><i class="fa fa-check"></i><b>11</b> Spatial and Temporal Data Management</a>
<ul>
<li class="chapter" data-level="" data-path="spatial-and-temporal-data-management.html"><a href="spatial-and-temporal-data-management.html#learning-objectives-10"><i class="fa fa-check"></i>Learning objectives</a></li>
<li class="chapter" data-level="" data-path="spatial-and-temporal-data-management.html"><a href="spatial-and-temporal-data-management.html#introduction-4"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="spatial-and-temporal-data-management.html"><a href="spatial-and-temporal-data-management.html#managing-spatial-data"><i class="fa fa-check"></i>Managing spatial data</a></li>
<li class="chapter" data-level="" data-path="spatial-and-temporal-data-management.html"><a href="spatial-and-temporal-data-management.html#managing-temporal-data"><i class="fa fa-check"></i>Managing temporal data</a></li>
<li class="chapter" data-level="" data-path="spatial-and-temporal-data-management.html"><a href="spatial-and-temporal-data-management.html#summary-11"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="spatial-and-temporal-data-management.html"><a href="spatial-and-temporal-data-management.html#key-terms-and-concepts-8"><i class="fa fa-check"></i>Key terms and concepts</a></li>
<li class="chapter" data-level="" data-path="spatial-and-temporal-data-management.html"><a href="spatial-and-temporal-data-management.html#references-and-additional-readings-6"><i class="fa fa-check"></i>References and additional readings</a></li>
<li class="chapter" data-level="" data-path="spatial-and-temporal-data-management.html"><a href="spatial-and-temporal-data-management.html#exercises-10"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="graph-databases.html"><a href="graph-databases.html"><i class="fa fa-check"></i><b>12</b> Graph Databases</a>
<ul>
<li class="chapter" data-level="" data-path="graph-databases.html"><a href="graph-databases.html#a-graph-database"><i class="fa fa-check"></i>A graph database</a></li>
<li class="chapter" data-level="" data-path="graph-databases.html"><a href="graph-databases.html#neo4j-a-graph-database-implementation"><i class="fa fa-check"></i>Neo4j – a graph database implementation</a></li>
<li class="chapter" data-level="" data-path="graph-databases.html"><a href="graph-databases.html#a-single-node"><i class="fa fa-check"></i>A single node</a></li>
<li class="chapter" data-level="" data-path="graph-databases.html"><a href="graph-databases.html#querying-a-node"><i class="fa fa-check"></i>Querying a node</a></li>
<li class="chapter" data-level="" data-path="graph-databases.html"><a href="graph-databases.html#a-relationship-between-nodes"><i class="fa fa-check"></i>A relationship between nodes</a></li>
<li class="chapter" data-level="" data-path="graph-databases.html"><a href="graph-databases.html#querying-an-mm-relationship-1"><i class="fa fa-check"></i>Querying an m:m relationship</a></li>
<li class="chapter" data-level="" data-path="graph-databases.html"><a href="graph-databases.html#recursive-relationships"><i class="fa fa-check"></i>Recursive relationships</a></li>
<li class="chapter" data-level="" data-path="graph-databases.html"><a href="graph-databases.html#summary-12"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="graph-databases.html"><a href="graph-databases.html#key-terms-and-concepts-9"><i class="fa fa-check"></i>Key terms and concepts</a></li>
<li class="chapter" data-level="" data-path="graph-databases.html"><a href="graph-databases.html#references-and-additional-resources"><i class="fa fa-check"></i>References and additional resources</a></li>
<li class="chapter" data-level="" data-path="graph-databases.html"><a href="graph-databases.html#exercises-11"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="xml-managing-data-exchange.html"><a href="xml-managing-data-exchange.html"><i class="fa fa-check"></i><b>13</b> XML: Managing Data Exchange</a>
<ul>
<li class="chapter" data-level="" data-path="xml-managing-data-exchange.html"><a href="xml-managing-data-exchange.html#sgml"><i class="fa fa-check"></i>SGML</a></li>
<li class="chapter" data-level="" data-path="xml-managing-data-exchange.html"><a href="xml-managing-data-exchange.html#xml"><i class="fa fa-check"></i>XML</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="organizational-intelligence.html"><a href="organizational-intelligence.html"><i class="fa fa-check"></i><b>14</b> Organizational Intelligence</a>
<ul>
<li class="chapter" data-level="" data-path="organizational-intelligence.html"><a href="organizational-intelligence.html#the-data-warehouse"><i class="fa fa-check"></i>The data warehouse</a></li>
<li class="chapter" data-level="" data-path="organizational-intelligence.html"><a href="organizational-intelligence.html#exploiting-data-stores"><i class="fa fa-check"></i>Exploiting data stores</a></li>
<li class="chapter" data-level="" data-path="organizational-intelligence.html"><a href="organizational-intelligence.html#olap"><i class="fa fa-check"></i>OLAP</a></li>
<li class="chapter" data-level="" data-path="organizational-intelligence.html"><a href="organizational-intelligence.html#data-mining"><i class="fa fa-check"></i>Data mining</a></li>
<li class="chapter" data-level="" data-path="organizational-intelligence.html"><a href="organizational-intelligence.html#summary-14"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="organizational-intelligence.html"><a href="organizational-intelligence.html#key-terms-and-concepts-11"><i class="fa fa-check"></i>Key terms and concepts</a></li>
<li class="chapter" data-level="" data-path="organizational-intelligence.html"><a href="organizational-intelligence.html#references-and-additional-readings-8"><i class="fa fa-check"></i>References and additional readings</a></li>
<li class="chapter" data-level="" data-path="organizational-intelligence.html"><a href="organizational-intelligence.html#exercises-13"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="introduction-to-r.html"><a href="introduction-to-r.html"><i class="fa fa-check"></i><b>15</b> Introduction to R</a>
<ul>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#the-r-project"><i class="fa fa-check"></i>The R project</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#database-access"><i class="fa fa-check"></i>Database access</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#timestamps"><i class="fa fa-check"></i>Timestamps</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#excel-files"><i class="fa fa-check"></i>Excel files</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#r-resources"><i class="fa fa-check"></i>R resources</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#r-and-data-analytics"><i class="fa fa-check"></i>R and data analytics</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#summary-15"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#key-terms-and-concepts-12"><i class="fa fa-check"></i>Key terms and concepts</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#references"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="" data-path="introduction-to-r.html"><a href="introduction-to-r.html#exercises-14"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="data-visualization-1.html"><a href="data-visualization-1.html"><i class="fa fa-check"></i><b>16</b> Data visualization</a>
<ul>
<li class="chapter" data-level="" data-path="data-visualization-1.html"><a href="data-visualization-1.html#visual-processing"><i class="fa fa-check"></i>Visual processing</a></li>
<li class="chapter" data-level="" data-path="data-visualization-1.html"><a href="data-visualization-1.html#the-grammar-of-graphics"><i class="fa fa-check"></i>The grammar of graphics</a></li>
<li class="chapter" data-level="" data-path="data-visualization-1.html"><a href="data-visualization-1.html#ggplot2"><i class="fa fa-check"></i>ggplot2</a></li>
<li class="chapter" data-level="" data-path="data-visualization-1.html"><a href="data-visualization-1.html#some-recipes"><i class="fa fa-check"></i>Some recipes</a></li>
<li class="chapter" data-level="" data-path="data-visualization-1.html"><a href="data-visualization-1.html#geographic-data"><i class="fa fa-check"></i>Geographic data</a></li>
<li class="chapter" data-level="" data-path="data-visualization-1.html"><a href="data-visualization-1.html#r-resources-1"><i class="fa fa-check"></i>R resources</a></li>
<li class="chapter" data-level="" data-path="data-visualization-1.html"><a href="data-visualization-1.html#summary-16"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="data-visualization-1.html"><a href="data-visualization-1.html#references-1"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="" data-path="data-visualization-1.html"><a href="data-visualization-1.html#exercises-15"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="text-mining-natural-language-processing.html"><a href="text-mining-natural-language-processing.html"><i class="fa fa-check"></i><b>17</b> Text mining &amp; natural language processing</a>
<ul>
<li class="chapter" data-level="" data-path="text-mining-natural-language-processing.html"><a href="text-mining-natural-language-processing.html#the-nature-of-language"><i class="fa fa-check"></i>The nature of language</a></li>
<li class="chapter" data-level="" data-path="text-mining-natural-language-processing.html"><a href="text-mining-natural-language-processing.html#levels-of-processing"><i class="fa fa-check"></i>Levels of processing</a></li>
<li class="chapter" data-level="" data-path="text-mining-natural-language-processing.html"><a href="text-mining-natural-language-processing.html#tokenization"><i class="fa fa-check"></i>Tokenization</a></li>
<li class="chapter" data-level="" data-path="text-mining-natural-language-processing.html"><a href="text-mining-natural-language-processing.html#sentiment-analysis"><i class="fa fa-check"></i>Sentiment analysis</a></li>
<li class="chapter" data-level="" data-path="text-mining-natural-language-processing.html"><a href="text-mining-natural-language-processing.html#corpus"><i class="fa fa-check"></i>Corpus</a></li>
<li class="chapter" data-level="" data-path="text-mining-natural-language-processing.html"><a href="text-mining-natural-language-processing.html#readability"><i class="fa fa-check"></i>Readability</a></li>
<li class="chapter" data-level="" data-path="text-mining-natural-language-processing.html"><a href="text-mining-natural-language-processing.html#preprocessing"><i class="fa fa-check"></i>Preprocessing</a></li>
<li class="chapter" data-level="" data-path="text-mining-natural-language-processing.html"><a href="text-mining-natural-language-processing.html#word-frequency-analysis"><i class="fa fa-check"></i>Word frequency analysis</a></li>
<li class="chapter" data-level="" data-path="text-mining-natural-language-processing.html"><a href="text-mining-natural-language-processing.html#co-occurrence-and-association"><i class="fa fa-check"></i>Co-occurrence and association</a></li>
<li class="chapter" data-level="" data-path="text-mining-natural-language-processing.html"><a href="text-mining-natural-language-processing.html#cluster-analysis"><i class="fa fa-check"></i>Cluster analysis</a></li>
<li class="chapter" data-level="" data-path="text-mining-natural-language-processing.html"><a href="text-mining-natural-language-processing.html#topic-modeling"><i class="fa fa-check"></i>Topic modeling</a></li>
<li class="chapter" data-level="" data-path="text-mining-natural-language-processing.html"><a href="text-mining-natural-language-processing.html#named-entity-recognition-ner"><i class="fa fa-check"></i>Named-entity recognition (NER)</a></li>
<li class="chapter" data-level="" data-path="text-mining-natural-language-processing.html"><a href="text-mining-natural-language-processing.html#future-developments"><i class="fa fa-check"></i>Future developments</a></li>
<li class="chapter" data-level="" data-path="text-mining-natural-language-processing.html"><a href="text-mining-natural-language-processing.html#summary-17"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="text-mining-natural-language-processing.html"><a href="text-mining-natural-language-processing.html#key-terms-and-concepts-13"><i class="fa fa-check"></i>Key terms and concepts</a></li>
<li class="chapter" data-level="" data-path="text-mining-natural-language-processing.html"><a href="text-mining-natural-language-processing.html#references-2"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="" data-path="text-mining-natural-language-processing.html"><a href="text-mining-natural-language-processing.html#exercises-16"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="cluster-computing.html"><a href="cluster-computing.html"><i class="fa fa-check"></i><b>18</b> Cluster computing</a>
<ul>
<li class="chapter" data-level="" data-path="cluster-computing.html"><a href="cluster-computing.html#a-paradigm-shift"><i class="fa fa-check"></i>A paradigm shift</a></li>
<li class="chapter" data-level="" data-path="cluster-computing.html"><a href="cluster-computing.html#the-drivers"><i class="fa fa-check"></i>The drivers</a></li>
<li class="chapter" data-level="" data-path="cluster-computing.html"><a href="cluster-computing.html#the-bottleneck-and-its-solution"><i class="fa fa-check"></i>The bottleneck and its solution</a></li>
<li class="chapter" data-level="" data-path="cluster-computing.html"><a href="cluster-computing.html#lambda-architecture"><i class="fa fa-check"></i>Lambda Architecture</a></li>
<li class="chapter" data-level="" data-path="cluster-computing.html"><a href="cluster-computing.html#hadoop"><i class="fa fa-check"></i>Hadoop</a></li>
<li class="chapter" data-level="" data-path="cluster-computing.html"><a href="cluster-computing.html#spark"><i class="fa fa-check"></i>Spark</a></li>
<li class="chapter" data-level="" data-path="cluster-computing.html"><a href="cluster-computing.html#summary-18"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="dashboards.html"><a href="dashboards.html"><i class="fa fa-check"></i><b>19</b> Dashboards</a>
<ul>
<li class="chapter" data-level="" data-path="dashboards.html"><a href="dashboards.html#the-value-of-dashboards"><i class="fa fa-check"></i>The value of dashboards</a></li>
<li class="chapter" data-level="" data-path="dashboards.html"><a href="dashboards.html#summary-19"><i class="fa fa-check"></i>Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="section-4-managing-organizational-memory.html"><a href="section-4-managing-organizational-memory.html"><i class="fa fa-check"></i>Section 4 Managing Organizational Memory</a></li>
<li class="chapter" data-level="20" data-path="data-structure-and-storage.html"><a href="data-structure-and-storage.html"><i class="fa fa-check"></i><b>20</b> Data Structure and Storage</a>
<ul>
<li class="chapter" data-level="" data-path="data-structure-and-storage.html"><a href="data-structure-and-storage.html#the-data-deluge"><i class="fa fa-check"></i>The data deluge</a></li>
<li class="chapter" data-level="" data-path="data-structure-and-storage.html"><a href="data-structure-and-storage.html#data-structures-1"><i class="fa fa-check"></i>Data structures</a></li>
<li class="chapter" data-level="" data-path="data-structure-and-storage.html"><a href="data-structure-and-storage.html#data-coding-standards"><i class="fa fa-check"></i>Data coding standards</a></li>
<li class="chapter" data-level="" data-path="data-structure-and-storage.html"><a href="data-structure-and-storage.html#data-storage-devices"><i class="fa fa-check"></i>Data storage devices</a></li>
<li class="chapter" data-level="" data-path="data-structure-and-storage.html"><a href="data-structure-and-storage.html#data-compression"><i class="fa fa-check"></i>Data compression</a></li>
<li class="chapter" data-level="" data-path="data-structure-and-storage.html"><a href="data-structure-and-storage.html#summary-20"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="data-structure-and-storage.html"><a href="data-structure-and-storage.html#key-terms-and-concepts-15"><i class="fa fa-check"></i>Key terms and concepts</a></li>
<li class="chapter" data-level="" data-path="data-structure-and-storage.html"><a href="data-structure-and-storage.html#exercises-19"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="data-processing-architectures.html"><a href="data-processing-architectures.html"><i class="fa fa-check"></i><b>21</b> Data Processing Architectures</a>
<ul>
<li class="chapter" data-level="" data-path="data-processing-architectures.html"><a href="data-processing-architectures.html#architectural-choices"><i class="fa fa-check"></i>Architectural choices</a></li>
<li class="chapter" data-level="" data-path="data-processing-architectures.html"><a href="data-processing-architectures.html#remote-job-entry"><i class="fa fa-check"></i>Remote job entry</a></li>
<li class="chapter" data-level="" data-path="data-processing-architectures.html"><a href="data-processing-architectures.html#personal-database"><i class="fa fa-check"></i>Personal database</a></li>
<li class="chapter" data-level="" data-path="data-processing-architectures.html"><a href="data-processing-architectures.html#clientserver"><i class="fa fa-check"></i>Client/server</a></li>
<li class="chapter" data-level="" data-path="data-processing-architectures.html"><a href="data-processing-architectures.html#cloud-computing"><i class="fa fa-check"></i>Cloud computing</a></li>
<li class="chapter" data-level="" data-path="data-processing-architectures.html"><a href="data-processing-architectures.html#distributed-database"><i class="fa fa-check"></i>Distributed database</a></li>
<li class="chapter" data-level="" data-path="data-processing-architectures.html"><a href="data-processing-architectures.html#distributed-data-access"><i class="fa fa-check"></i>Distributed data access</a></li>
<li class="chapter" data-level="" data-path="data-processing-architectures.html"><a href="data-processing-architectures.html#distributed-database-design"><i class="fa fa-check"></i>Distributed database design</a></li>
<li class="chapter" data-level="" data-path="data-processing-architectures.html"><a href="data-processing-architectures.html#summary-21"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="data-processing-architectures.html"><a href="data-processing-architectures.html#key-terms-and-concepts-16"><i class="fa fa-check"></i>Key terms and concepts</a></li>
<li class="chapter" data-level="" data-path="data-processing-architectures.html"><a href="data-processing-architectures.html#references-and-additional-readings-10"><i class="fa fa-check"></i>References and additional readings</a></li>
<li class="chapter" data-level="" data-path="data-processing-architectures.html"><a href="data-processing-architectures.html#exercises-20"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="sql-and-java.html"><a href="sql-and-java.html"><i class="fa fa-check"></i><b>22</b> SQL and Java</a>
<ul>
<li class="chapter" data-level="" data-path="sql-and-java.html"><a href="sql-and-java.html#jdbc"><i class="fa fa-check"></i>JDBC</a></li>
<li class="chapter" data-level="" data-path="sql-and-java.html"><a href="sql-and-java.html#java-ee"><i class="fa fa-check"></i>Java EE</a></li>
<li class="chapter" data-level="" data-path="sql-and-java.html"><a href="sql-and-java.html#using-sql-within-java"><i class="fa fa-check"></i>Using SQL within Java</a></li>
<li class="chapter" data-level="" data-path="sql-and-java.html"><a href="sql-and-java.html#javaserver-pages-jsp"><i class="fa fa-check"></i>JavaServer Pages (JSP)</a></li>
<li class="chapter" data-level="" data-path="sql-and-java.html"><a href="sql-and-java.html#summary-22"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="sql-and-java.html"><a href="sql-and-java.html#key-terms-and-concepts-17"><i class="fa fa-check"></i>Key terms and concepts</a></li>
<li class="chapter" data-level="" data-path="sql-and-java.html"><a href="sql-and-java.html#references-and-additional-readings-11"><i class="fa fa-check"></i>References and additional readings</a></li>
<li class="chapter" data-level="" data-path="sql-and-java.html"><a href="sql-and-java.html#exercises-21"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="data-integrity.html"><a href="data-integrity.html"><i class="fa fa-check"></i><b>23</b> Data Integrity</a>
<ul>
<li class="chapter" data-level="" data-path="data-integrity.html"><a href="data-integrity.html#introduction-9"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="data-integrity.html"><a href="data-integrity.html#transaction-management"><i class="fa fa-check"></i>Transaction management</a></li>
<li class="chapter" data-level="" data-path="data-integrity.html"><a href="data-integrity.html#protecting-existence"><i class="fa fa-check"></i>Protecting existence</a></li>
<li class="chapter" data-level="" data-path="data-integrity.html"><a href="data-integrity.html#maintaining-data-quality"><i class="fa fa-check"></i>Maintaining data quality</a></li>
<li class="chapter" data-level="" data-path="data-integrity.html"><a href="data-integrity.html#ensuring-confidentiality"><i class="fa fa-check"></i>Ensuring confidentiality</a></li>
<li class="chapter" data-level="" data-path="data-integrity.html"><a href="data-integrity.html#summary-23"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="data-integrity.html"><a href="data-integrity.html#key-terms-and-concepts-18"><i class="fa fa-check"></i>Key terms and concepts</a></li>
<li class="chapter" data-level="" data-path="data-integrity.html"><a href="data-integrity.html#exercises-22"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="data-administration.html"><a href="data-administration.html"><i class="fa fa-check"></i><b>24</b> Data Administration</a>
<ul>
<li class="chapter" data-level="" data-path="data-administration.html"><a href="data-administration.html#introduction-10"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="data-administration.html"><a href="data-administration.html#the-chief-data-officer"><i class="fa fa-check"></i>The Chief Data Officer</a></li>
<li class="chapter" data-level="" data-path="data-administration.html"><a href="data-administration.html#management-of-the-database-environment"><i class="fa fa-check"></i>Management of the database environment</a></li>
<li class="chapter" data-level="" data-path="data-administration.html"><a href="data-administration.html#data-administration-1"><i class="fa fa-check"></i>Data administration</a></li>
<li class="chapter" data-level="" data-path="data-administration.html"><a href="data-administration.html#database-management-systems-dbmss"><i class="fa fa-check"></i>Database management systems (DBMSs)</a></li>
<li class="chapter" data-level="" data-path="data-administration.html"><a href="data-administration.html#groupware-1"><i class="fa fa-check"></i>Groupware</a></li>
<li class="chapter" data-level="" data-path="data-administration.html"><a href="data-administration.html#data-integration"><i class="fa fa-check"></i>Data integration</a></li>
<li class="chapter" data-level="" data-path="data-administration.html"><a href="data-administration.html#conclusion-9"><i class="fa fa-check"></i>Conclusion</a></li>
<li class="chapter" data-level="" data-path="data-administration.html"><a href="data-administration.html#summary-24"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="" data-path="data-administration.html"><a href="data-administration.html#key-terms-and-concepts-19"><i class="fa fa-check"></i>Key terms and concepts</a></li>
<li class="chapter" data-level="" data-path="data-administration.html"><a href="data-administration.html#references-and-additional-readings-12"><i class="fa fa-check"></i>References and additional readings</a></li>
<li class="chapter" data-level="" data-path="data-administration.html"><a href="data-administration.html#exercises-23"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="text-mining-natural-language-processing" class="section level1" number="17">
<h1><span class="header-section-number">Chapter 17</span> Text mining &amp; natural language processing</h1>
<blockquote>
<p>From now on I will consider a language to be a set (finite or
infinite) of sentences, each finite in length and constructed out of a
finite set of elements. All natural languages in their spoken or
written form are languages in this sense.</p>
<p>Noam Chomsky, <em>Syntactic Structures</em></p>
</blockquote>
<div id="learning-objectives-16" class="section level3 unnumbered">
<h3>Learning objectives</h3>
<p>Students completing this chapter will:</p>
<ul>
<li><p>Have a realistic understanding of the capabilities of current text
mining and NLP software;</p></li>
<li><p>Be able to use R and associated packages for text mining and NLP.</p></li>
</ul>
</div>
<div id="the-nature-of-language" class="section level2 unnumbered">
<h2>The nature of language</h2>
<p>Language enables humans to cooperate through information exchange. We
typically associate language with sound and writing, but gesturing,
which is older than speech, is also a means of collaboration. The
various dialects of sign languages are effective tools for visual
communication. Some species, such as ants and bees, exchange information
using chemical substances known as pheromones. Of all the species,
humans have developed the most complex system for cooperation, starting
with gestures and progressing to digital technology, with language being
the core of our ability to work collectively.</p>
<p>Natural language processing (NLP) focuses on developing and implementing
software that enables computers to handle large scale processing of
language in a variety of forms, such as written and spoken. While it is
a relatively easy task for computers to process numeric information,
language is far more difficult because of the flexibility with which it
is used, even when grammar and syntax are precisely obeyed. There is an
inherent ambiguity of written and spoken speech. For example, the word
“set” can be a noun, verb, or adjective, and the <em>Oxford English
Dictionary</em> defines over 40 different meanings. Irregularities in
language, both in its structure and use, and ambiguities in meaning make
NLP a challenging task. Be forewarned. Don’t expect NLP to provide the
same level of exactness and starkness as numeric processing. NLP output
can be messy, imprecise, and confusing – just like the language that
goes into an NLP program. One of the well-known maxims of information
processing is “garbage-in, garbage-out.” While language is not garbage,
we can certainly observe that “language-in, language-out” is a truism.
You can’t start with something that is marginally ambiguous and expect a
computer to turn it into a precise statement. Legal and religious
scholars can spend years learning how to interpret a text and still
reach different conclusions as to meaning.</p>
<p>NLP, despite its limitations, enables humans to process large volumes of
language data (e.g., text) quickly and to identify patterns and features
that might be useful. A well-educated human with domain knowledge
specific to the same data might make more sense of these data, but it
might take months or years. For example, a firm might receive over a
1,000 tweets, 500 Facebook mentions, and 20 blog references in a day. It
needs NLP to identify within minutes or hours which of these many
messages might need human action.</p>
<p>Text mining and NLP overlap in their capabilities and goals. The
ultimate objective is to extract useful and valuable information from
text using analytical methods and NLP. Simply counting words in a
document is a an example of text mining because it requires minimal NLP
technology, other than separating text into words. Whereas, recognizing
entities in a document requires prior extensive machine learning and
more intensive NLP knowledge. Whether you call it text mining or NLP,
you are processing natural language. We will use the terms somewhat
interchangeably in this chapter.</p>
<p>The human brain has a special capability for learning and processing
languages and reconciling ambiguities,<a href="#fn56" class="footnote-ref" id="fnref56"><sup>56</sup></a> and it is a skill
we have yet to transfer to computers. NLP can be a good servant, but
enter its realm with realistic expectations of what is achievable with
the current state-of-the-art.</p>
</div>
<div id="levels-of-processing" class="section level2 unnumbered">
<h2>Levels of processing</h2>
<p>There are three levels to consider when processing language.</p>
<div id="semantics" class="section level3 unnumbered">
<h3>Semantics</h3>
<p>Semantics focuses on the meaning of words and the interactions between
words to form larger units of meaning (such as sentences). Words in
isolation often provide little information. We normally need to read or
hear a sentence to understand the sender’s intent. One word can change
the meaning of a sentence (e.g., “Help needed versus Help not needed”).
It is typically an entire sentence that conveys meaning. Of course,
elaborate ideas or commands can require many sentences.</p>
</div>
<div id="discourse" class="section level3 unnumbered">
<h3>Discourse</h3>
<p>Building on semantic analysis, discourse analysis aims to determine the
relationships between sentences in a communication, such as a
conversation, consisting of multiple sentences in a particular order.
Most human communications are a series of connected sentences that
collectively disclose the sender’s goals. Typically, interspersed in a
conversation are one or more sentences from one or more receivers as
they try to understand the sender’s purpose and maybe interject their
thoughts and goals into the discussion. The points and counterpoints of
a blog are an example of such a discourse. As you might imagine, making
sense of discourse is frequently more difficult, for both humans and
machines, than comprehending a single sentence. However, the braiding of
question and answer in a discourse, can sometimes help to reduce
ambiguity.</p>
</div>
<div id="pragmatics" class="section level3 unnumbered">
<h3>Pragmatics</h3>
<p>Finally, pragmatics studies how context, world knowledge, language
conventions, and other abstract properties contribute to the meaning of
human conversation. Our shared experiences and knowledge often help us
to make sense of situations. We derive meaning from the manner of the
discourse, where it takes place, its time and length, who else is
involved, and so forth. Thus, we usually find it much easier to
communicate with those with whom we share a common culture, history, and
socioeconomic status because the great collection of knowledge we
jointly share assists in overcoming ambiguity.</p>
</div>
</div>
<div id="tokenization" class="section level2 unnumbered">
<h2>Tokenization</h2>
<p>Tokenization is the process of breaking a document into chunks (e.g.,
words), which are called tokens. Whitespaces (e.g., spaces and tabs) are
used to determine where a break occurs. Tokenization typically creates a
<em>bag of words</em> for subsequent processing. Many text mining functions use
words as the foundation for analysis.</p>
<div id="counting-words" class="section level3 unnumbered">
<h3>Counting words</h3>
<p>To count the number of words in a string, simply count the number of
times there are one or more consecutive spaces using the pattern
“<span class="math display">\[:space:\]</span>+” and then add one, because the last word is not followed
by a space.</p>
<div class="sourceCode" id="cb483"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb483-1"><a href="text-mining-natural-language-processing.html#cb483-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(stringr)</span>
<span id="cb483-2"><a href="text-mining-natural-language-processing.html#cb483-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str_count</span>(<span class="st">&quot;The dead batteries were given out free of charge&quot;</span>, <span class="st">&quot;[:space:]+&quot;</span>) <span class="sc">+</span> <span class="dv">1</span></span></code></pre></div>
<pre><code>## [1] 9</code></pre>
</div>
</div>
<div id="sentiment-analysis" class="section level2 unnumbered">
<h2>Sentiment analysis</h2>
<p>Sentiment analysis is a popular and simple method of measuring aggregate
feeling. In its simplest form, it is computed by giving a score of +1 to
each “positive” word and -1 to each “negative” word and summing the
total to get a sentiment score. A text is decomposed into words. Each
word is then checked against a list to find its score (i.e., +1 or -1),
and if the word is not in the list, it doesn’t score.</p>
<p>A major shortcoming of sentiment analysis is that irony (e.g., “The name
of Britain’s biggest dog (until it died) was Tiny”) and sarcasm (e.g.,
“I started out with nothing and still have most of it left”) are usually
misclassified. Also, a phrase such as “not happy” might be scored as +1
by a sentiment analysis program that simply examines each word and not
those around it.</p>
<p>The <strong>sentimentr package</strong> offers an advanced implementation of
sentiment analysis. It is based on a polarity table, in which a word and
its polarity score (e.g., -1 for a negative word) are recorded. The
default polarity table is provided by the syuzhet package. You can
create a polarity table suitable for your context, and you are not
restricted to 1 or -1 for a word’s polarity score. Here are the first
few rows of the default polarity table.</p>
<div class="sourceCode" id="cb485"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb485-1"><a href="text-mining-natural-language-processing.html#cb485-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sentimentr)</span>
<span id="cb485-2"><a href="text-mining-natural-language-processing.html#cb485-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(syuzhet)</span>
<span id="cb485-3"><a href="text-mining-natural-language-processing.html#cb485-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">get_sentiment_dictionary</span>())</span></code></pre></div>
<pre><code>##          word value
## 1     abandon -0.75
## 2   abandoned -0.50
## 3   abandoner -0.25
## 4 abandonment -0.25
## 5    abandons -1.00
## 6    abducted -1.00</code></pre>
<p>In addition, sentimentr supports valence shifters, which are words that
alter or intensify the meaning of a polarizing word (i.e., a word
appearing in the polarity table) appearing in the text or document under
examination. Each word has a value to indicate how to interpret its
effect (negators (1), amplifiers(2), de-amplifiers (3), and conjunction
(4).</p>
<p>Now, let’s see how we use the <strong>sentiment function</strong>. We’ll start with
an example that does not use valence shifters, in which case we specify
that the sentiment function should not look for valence words before or
after any polarizing word. We indicate this by setting n.before and
n.after to 0. Our sample text consists of several sentences, as shown in
the following code.</p>
<div class="sourceCode" id="cb487"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb487-1"><a href="text-mining-natural-language-processing.html#cb487-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sentimentr)</span>
<span id="cb487-2"><a href="text-mining-natural-language-processing.html#cb487-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(syuzhet)</span>
<span id="cb487-3"><a href="text-mining-natural-language-processing.html#cb487-3" aria-hidden="true" tabindex="-1"></a>sample <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;You&#39;re awesome and I love you&quot;</span>, <span class="st">&quot;I hate and hate and hate.</span></span>
<span id="cb487-4"><a href="text-mining-natural-language-processing.html#cb487-4" aria-hidden="true" tabindex="-1"></a><span class="st">So angry. Die!&quot;</span>, <span class="st">&quot;Impressed and amazed: you are peerless in your</span></span>
<span id="cb487-5"><a href="text-mining-natural-language-processing.html#cb487-5" aria-hidden="true" tabindex="-1"></a><span class="st">achievement of unparalleled mediocrity.&quot;</span>)</span>
<span id="cb487-6"><a href="text-mining-natural-language-processing.html#cb487-6" aria-hidden="true" tabindex="-1"></a><span class="fu">sentiment</span>(sample, <span class="at">n.before=</span><span class="dv">0</span>, <span class="at">n.after=</span><span class="dv">0</span>, <span class="at">amplifier.weight=</span><span class="dv">0</span>)</span></code></pre></div>
<pre><code>##    element_id sentence_id word_count  sentiment
## 1:          1           1          6  0.5511352
## 2:          2           1          6 -0.9185587
## 3:          2           2          2 -0.5303301
## 4:          2           3          1 -0.7500000
## 5:          3           1         12  0.6495191</code></pre>
<p>Notice that the sentiment function breaks each element (the text between
quotes in this case) into sentences, identifies each sentence in an
element, and computes the word count for each of these sentences. The
sentiment score is the sum of the polarity scores divided by the square
root of the number of words in the associated sentence.</p>
<p>To get the overall sentiment for the sample text, use:</p>
<div class="sourceCode" id="cb489"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb489-1"><a href="text-mining-natural-language-processing.html#cb489-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sentimentr)</span>
<span id="cb489-2"><a href="text-mining-natural-language-processing.html#cb489-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(syuzhet)</span>
<span id="cb489-3"><a href="text-mining-natural-language-processing.html#cb489-3" aria-hidden="true" tabindex="-1"></a>sample <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;You&#39;re awesome and I love you&quot;</span>, <span class="st">&quot;I hate and hate and hate.So angry. Die!&quot;</span>, <span class="st">&quot;Impressed and amazed: you are peerless in your achievement of unparalleled mediocrity.&quot;</span>)</span>
<span id="cb489-4"><a href="text-mining-natural-language-processing.html#cb489-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">sentiment</span>(sample,</span>
<span id="cb489-5"><a href="text-mining-natural-language-processing.html#cb489-5" aria-hidden="true" tabindex="-1"></a><span class="at">n.before=</span><span class="dv">0</span>, <span class="at">n.after=</span><span class="dv">0</span>)</span>
<span id="cb489-6"><a href="text-mining-natural-language-processing.html#cb489-6" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(y<span class="sc">$</span>sentiment)</span></code></pre></div>
<pre><code>## [1] -0.1996469</code></pre>
<p>When a <strong>valence shift</strong> is detected before or after a polarizing word,
its effect is incorporated in the sentiment calculation. The size of the
effect is indicated by the amplifier.weight, a sentiment function
parameter with a value between 0 and 1. The weight amplifies or
de-amplifies by multiplying the polarized terms by 1 + the amplifier
weight. A <strong><em>negator</em></strong> flips the sign of a polarizing word. A
<strong><em>conjunction</em></strong> amplifies the current clause and down weights the
prior clause. Some examples in the following table illustrate the
results of applying the code to a variety of input text. The polarities
are -1 (crazy) and 1 (love). There is a negator (not), two amplifiers
(very and much), and a conjunction (but). Contractions are treated as
amplifiers and so get weights based on the contraction (.9 in this case)
and amplification (.8) in this case.</p>
<div class="sourceCode" id="cb491"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb491-1"><a href="text-mining-natural-language-processing.html#cb491-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sentimentr)</span>
<span id="cb491-2"><a href="text-mining-natural-language-processing.html#cb491-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(syuzhet)</span>
<span id="cb491-3"><a href="text-mining-natural-language-processing.html#cb491-3" aria-hidden="true" tabindex="-1"></a>sample <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;You&#39;re awesome and I love you&quot;</span>, <span class="st">&quot;I hate and hate and hate.</span></span>
<span id="cb491-4"><a href="text-mining-natural-language-processing.html#cb491-4" aria-hidden="true" tabindex="-1"></a><span class="st">So angry. Die!&quot;</span>, <span class="st">&quot;Impressed and amazed: you are peerless in your</span></span>
<span id="cb491-5"><a href="text-mining-natural-language-processing.html#cb491-5" aria-hidden="true" tabindex="-1"></a><span class="st">achievement of unparalleled mediocrity.&quot;</span>)</span>
<span id="cb491-6"><a href="text-mining-natural-language-processing.html#cb491-6" aria-hidden="true" tabindex="-1"></a><span class="fu">sentiment</span>(sample, <span class="at">n.before =</span> <span class="dv">2</span>, <span class="at">n.after =</span> <span class="dv">2</span>, <span class="at">amplifier.weight=</span>.<span class="dv">8</span>, <span class="at">but.weight =</span> .<span class="dv">9</span>)</span></code></pre></div>
<pre><code>##    element_id sentence_id word_count  sentiment
## 1:          1           1          6  0.5511352
## 2:          2           1          6 -0.9185587
## 3:          2           2          2 -0.5303301
## 4:          2           3          1 -0.7500000
## 5:          3           1         12  0.6495191</code></pre>
<table>
<thead>
<tr class="header">
<th align="left"><strong>Type</strong></th>
<th align="left"><strong>Code</strong></th>
<th align="left"><strong>Text</strong></th>
<th align="left"><strong>Sentiment</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="left"></td>
<td align="left">You’re crazy, and I love you.</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="left">Negator</td>
<td align="left">1</td>
<td align="left">You’re not crazy, and I love you.</td>
<td align="left">0.57</td>
</tr>
<tr class="odd">
<td align="left">Amplifier</td>
<td align="left">2</td>
<td align="left">You’re crazy, and I love you very much.</td>
<td align="left">0.21</td>
</tr>
<tr class="even">
<td align="left">De-amplifier</td>
<td align="left">3</td>
<td align="left">You’re slightly crazy, and I love you.</td>
<td align="left">0.23</td>
</tr>
<tr class="odd">
<td align="left">Conjunction</td>
<td align="left">4</td>
<td align="left">You’re crazy, but I love you.</td>
<td align="left">0.45</td>
</tr>
</tbody>
</table>
<blockquote>
<p>❓<strong>Skill builder</strong></p>
<p>Run the following R code and comment on how sensitive sentiment
analysis is to the n.before and n.after parameters.</p>
</blockquote>
<div class="sourceCode" id="cb493"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb493-1"><a href="text-mining-natural-language-processing.html#cb493-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sentimentr) </span>
<span id="cb493-2"><a href="text-mining-natural-language-processing.html#cb493-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(syuzhet)</span>
<span id="cb493-3"><a href="text-mining-natural-language-processing.html#cb493-3" aria-hidden="true" tabindex="-1"></a>sample <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;You&#39;re not crazy and I love you very much.&quot;</span>)</span>
<span id="cb493-4"><a href="text-mining-natural-language-processing.html#cb493-4" aria-hidden="true" tabindex="-1"></a><span class="fu">sentiment</span>(sample, <span class="at">n.before =</span> <span class="dv">4</span>, <span class="at">n.after=</span><span class="dv">3</span>, <span class="at">amplifier.weight=</span><span class="dv">1</span>) </span></code></pre></div>
<pre><code>##    element_id sentence_id word_count sentiment
## 1:          1           1          9   0.24975</code></pre>
<div class="sourceCode" id="cb495"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb495-1"><a href="text-mining-natural-language-processing.html#cb495-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sentiment</span>(sample, <span class="at">n.before =</span> <span class="cn">Inf</span>, <span class="at">n.after=</span><span class="cn">Inf</span>, <span class="at">amplifier.weight=</span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>##    element_id sentence_id word_count sentiment
## 1:          1           1          9         0</code></pre>
<blockquote>
<p>What are the correct polarities for each word, and weights for
negators, amplifiers and so on? It is a judgment call and one that is
difficult to justify.</p>
</blockquote>
<p>Sentiment analysis has given you an idea of some of the issues
surrounding text mining. Let’s now look at the topic in more depth and
explore some of the tools available in <strong>tm</strong>, a general purpose text
mining package for R. We will also use a few other R packages which
support text mining and displaying the results.</p>
</div>
<div id="corpus" class="section level2 unnumbered">
<h2>Corpus</h2>
<p>A collection of text is called a corpus. It is common to use N for the
<em>corpus size</em>, the number of tokens, and V for the <em>vocabulary</em>, the
number of distinct tokens.</p>
<p>In the following examples, our corpus consists of Warren Buffett’s
annual letters to the shareholders of Berkshire Hathaway<a href="#fn57" class="footnote-ref" id="fnref57"><sup>57</sup></a>
for the period 1998-2012.<a href="#fn58" class="footnote-ref" id="fnref58"><sup>58</sup></a> The letters, available in html
or pdf, were converted to separate text files using Abbyy Fine Reader.
Tables, page numbers, and graphics were removed during the conversion.</p>
<p>The following R code sets up a loop to read each of the letters and add
it to a data frame. When all the letters have been read, they are turned
into a corpus.</p>
<div class="sourceCode" id="cb497"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb497-1"><a href="text-mining-natural-language-processing.html#cb497-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(stringr)</span>
<span id="cb497-2"><a href="text-mining-natural-language-processing.html#cb497-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tm)</span>
<span id="cb497-3"><a href="text-mining-natural-language-processing.html#cb497-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(NLP)</span>
<span id="cb497-4"><a href="text-mining-natural-language-processing.html#cb497-4" aria-hidden="true" tabindex="-1"></a><span class="co">#set up a data frame to hold up to 100 letters</span></span>
<span id="cb497-5"><a href="text-mining-natural-language-processing.html#cb497-5" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span>  <span class="fu">data.frame</span>(<span class="at">num=</span><span class="dv">100</span>)</span>
<span id="cb497-6"><a href="text-mining-natural-language-processing.html#cb497-6" aria-hidden="true" tabindex="-1"></a>begin <span class="ot">&lt;-</span>  <span class="dv">1998</span> <span class="co"># date of first letter</span></span>
<span id="cb497-7"><a href="text-mining-natural-language-processing.html#cb497-7" aria-hidden="true" tabindex="-1"></a>i <span class="ot">&lt;-</span>  begin</span>
<span id="cb497-8"><a href="text-mining-natural-language-processing.html#cb497-8" aria-hidden="true" tabindex="-1"></a><span class="co"># read the letters</span></span>
<span id="cb497-9"><a href="text-mining-natural-language-processing.html#cb497-9" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> (i <span class="sc">&lt;</span> <span class="dv">2013</span>) {</span>
<span id="cb497-10"><a href="text-mining-natural-language-processing.html#cb497-10" aria-hidden="true" tabindex="-1"></a> y <span class="ot">&lt;-</span> <span class="fu">as.character</span>(i)</span>
<span id="cb497-11"><a href="text-mining-natural-language-processing.html#cb497-11" aria-hidden="true" tabindex="-1"></a> <span class="co"># create the file name</span></span>
<span id="cb497-12"><a href="text-mining-natural-language-processing.html#cb497-12" aria-hidden="true" tabindex="-1"></a> f <span class="ot">&lt;-</span> <span class="fu">str_c</span>(<span class="st">&#39;http://www.richardtwatson.com/BuffettLetters/&#39;</span>,y, <span class="st">&#39;ltr.txt&#39;</span>,<span class="at">delim=</span><span class="st">&#39;&#39;</span>)</span>
<span id="cb497-13"><a href="text-mining-natural-language-processing.html#cb497-13" aria-hidden="true" tabindex="-1"></a> <span class="co"># read the letter as on large string</span></span>
<span id="cb497-14"><a href="text-mining-natural-language-processing.html#cb497-14" aria-hidden="true" tabindex="-1"></a> d <span class="ot">&lt;-</span>  <span class="fu">readChar</span>(f,<span class="at">nchars=</span><span class="fl">1e6</span>)</span>
<span id="cb497-15"><a href="text-mining-natural-language-processing.html#cb497-15" aria-hidden="true" tabindex="-1"></a> <span class="co"># add letter to the data frame</span></span>
<span id="cb497-16"><a href="text-mining-natural-language-processing.html#cb497-16" aria-hidden="true" tabindex="-1"></a> df[i<span class="sc">-</span>begin<span class="sc">+</span><span class="dv">1</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span>  i</span>
<span id="cb497-17"><a href="text-mining-natural-language-processing.html#cb497-17" aria-hidden="true" tabindex="-1"></a> df[i<span class="sc">-</span>begin<span class="sc">+</span><span class="dv">1</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span>  d</span>
<span id="cb497-18"><a href="text-mining-natural-language-processing.html#cb497-18" aria-hidden="true" tabindex="-1"></a> i <span class="ot">&lt;-</span>  i <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb497-19"><a href="text-mining-natural-language-processing.html#cb497-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb497-20"><a href="text-mining-natural-language-processing.html#cb497-20" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(df) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&#39;doc_id&#39;</span>, <span class="st">&#39;text&#39;</span>)</span>
<span id="cb497-21"><a href="text-mining-natural-language-processing.html#cb497-21" aria-hidden="true" tabindex="-1"></a><span class="co"># create the corpus</span></span>
<span id="cb497-22"><a href="text-mining-natural-language-processing.html#cb497-22" aria-hidden="true" tabindex="-1"></a>letters <span class="ot">&lt;-</span>  <span class="fu">Corpus</span>(<span class="fu">DataframeSource</span>(df))</span></code></pre></div>
</div>
<div id="readability" class="section level2 unnumbered">
<h2>Readability</h2>
<p>There are several approaches to estimating the readability of a
selection of text. They are usually based on counting the words in each
sentence and the number of syllables in each word. For example, the
Flesch-Kincaid method uses the formula:</p>
<span class="math display">\[\begin{equation}
(11.8 * syllables\: per\: word) + (0.39 * words\:per\:sentence) - 15.59
\end{equation}\]</span>
<p>It estimates the grade-level or years of education required of the
reader. The bands are:</p>
<table>
<caption>Readability score and grade level</caption>
<thead>
<tr class="header">
<th align="left">Score</th>
<th align="left">Education level</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">13-16</td>
<td align="left">Undergraduate</td>
</tr>
<tr class="even">
<td align="left">16-18</td>
<td align="left">Masters</td>
</tr>
<tr class="odd">
<td align="left">19-</td>
<td align="left">PhD</td>
</tr>
</tbody>
</table>
<p>The R package koRpus has a number of methods for calculating readability
scores. You first need to tokenize the text using the package’s tokenize
function. Then complete the calculation.</p>
<div class="sourceCode" id="cb498"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb498-1"><a href="text-mining-natural-language-processing.html#cb498-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(koRpus)</span>
<span id="cb498-2"><a href="text-mining-natural-language-processing.html#cb498-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(koRpus.lang.en)</span>
<span id="cb498-3"><a href="text-mining-natural-language-processing.html#cb498-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(sylly)</span>
<span id="cb498-4"><a href="text-mining-natural-language-processing.html#cb498-4" aria-hidden="true" tabindex="-1"></a><span class="co">#tokenize the first letter in the corpus after converting to character vector</span></span>
<span id="cb498-5"><a href="text-mining-natural-language-processing.html#cb498-5" aria-hidden="true" tabindex="-1"></a>txt <span class="ot">&lt;-</span>  letters[[<span class="dv">1</span>]][<span class="dv">1</span>] <span class="co"># first element in the list</span></span>
<span id="cb498-6"><a href="text-mining-natural-language-processing.html#cb498-6" aria-hidden="true" tabindex="-1"></a>tagged.text <span class="ot">&lt;-</span> <span class="fu">tokenize</span>(<span class="fu">as.character</span>(txt),<span class="at">format=</span><span class="st">&quot;obj&quot;</span>,<span class="at">lang=</span><span class="st">&quot;en&quot;</span>)</span>
<span id="cb498-7"><a href="text-mining-natural-language-processing.html#cb498-7" aria-hidden="true" tabindex="-1"></a><span class="co"># score</span></span>
<span id="cb498-8"><a href="text-mining-natural-language-processing.html#cb498-8" aria-hidden="true" tabindex="-1"></a><span class="fu">readability</span>(tagged.text, <span class="at">hyphen=</span><span class="cn">NULL</span>,<span class="at">index=</span><span class="st">&quot;FORCAST&quot;</span>, <span class="at">quiet =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## 
## FORCAST
##   Parameters: default 
##        Grade: 9.89 
##          Age: 14.89 
## 
## Text language: en</code></pre>
</div>
<div id="preprocessing" class="section level2 unnumbered">
<h2>Preprocessing</h2>
<p>Before commencing analysis, a text file typically needs to be prepared
for processing. Several steps are usually taken.</p>
<div id="case-conversion" class="section level3 unnumbered">
<h3>Case conversion</h3>
<p>For comparison purposes, all text should be of the same case.
Conventionally, the choice is to convert to all lower case. First
convert to UTF-8.</p>
<div class="sourceCode" id="cb500"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb500-1"><a href="text-mining-natural-language-processing.html#cb500-1" aria-hidden="true" tabindex="-1"></a>content_transformer <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">iconv</span>(x, <span class="at">to=</span><span class="st">&#39;UTF-8-MAC&#39;</span>, <span class="at">sub=</span><span class="st">&#39;byte&#39;</span>)</span>
<span id="cb500-2"><a href="text-mining-natural-language-processing.html#cb500-2" aria-hidden="true" tabindex="-1"></a>clean.letters <span class="ot">&lt;-</span> <span class="fu">tm_map</span>(letters, content_transformer)</span>
<span id="cb500-3"><a href="text-mining-natural-language-processing.html#cb500-3" aria-hidden="true" tabindex="-1"></a>clean.letters <span class="ot">&lt;-</span> <span class="fu">tm_map</span>(letters,tolower)</span></code></pre></div>
</div>
<div id="punctuation-removal" class="section level3 unnumbered">
<h3>Punctuation removal</h3>
<p>Punctuation is usually removed when the focus is just on the words in a
text and not on higher level elements such as sentences and paragraphs.</p>
<div class="sourceCode" id="cb501"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb501-1"><a href="text-mining-natural-language-processing.html#cb501-1" aria-hidden="true" tabindex="-1"></a>clean.letters <span class="ot">&lt;-</span> <span class="fu">tm_map</span>(clean.letters,removePunctuation)</span></code></pre></div>
</div>
<div id="number-removal" class="section level3 unnumbered">
<h3>Number removal</h3>
<p>You might also want to remove all numbers.</p>
<div class="sourceCode" id="cb502"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb502-1"><a href="text-mining-natural-language-processing.html#cb502-1" aria-hidden="true" tabindex="-1"></a>clean.letters <span class="ot">&lt;-</span> <span class="fu">tm_map</span>(clean.letters,removeNumbers)</span></code></pre></div>
</div>
<div id="stripping-extra-white-spaces" class="section level3 unnumbered">
<h3>Stripping extra white spaces</h3>
<p>Removing extra spaces, tabs, and such is another common preprocessing
action.</p>
<div class="sourceCode" id="cb503"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb503-1"><a href="text-mining-natural-language-processing.html#cb503-1" aria-hidden="true" tabindex="-1"></a>clean.letters <span class="ot">&lt;-</span> <span class="fu">tm_map</span>(clean.letters,stripWhitespace)</span></code></pre></div>
<blockquote>
<p>❓<strong>Skill builder</strong></p>
<p>Redo the readability calculation after executing the preprocessing
steps described in the previous section. What do you observe?</p>
</blockquote>
</div>
<div id="stop-word-filtering" class="section level3 unnumbered">
<h3>Stop word filtering</h3>
<p>Stop words are short common words that can be removed from a text
without affecting the results of an analysis. Though there is no
commonly agreed upon list of stop works, typically included are <em>the,
is, be, and, but, to,</em> and <em>on</em>. Stop word lists are typically all
lowercase, thus you should convert to lowercase before removing stop
words. Each language has a set of stop words. In the following sample
code, we use the SMART list of English stop words.<a href="#fn59" class="footnote-ref" id="fnref59"><sup>59</sup></a></p>
<div class="sourceCode" id="cb504"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb504-1"><a href="text-mining-natural-language-processing.html#cb504-1" aria-hidden="true" tabindex="-1"></a>clean.letters <span class="ot">&lt;-</span> <span class="fu">tm_map</span>(clean.letters,removeWords,<span class="fu">stopwords</span>(<span class="st">&quot;SMART&quot;</span>))</span></code></pre></div>
</div>
<div id="specific-word-removal" class="section level3 unnumbered">
<h3>Specific word removal</h3>
<p>There can also specify particular words to be removed via a character
vector. For instance, you might not be interested in tracking references
to <em>Berkshire Hathaway</em> in Buffett’s letters. You can set up a
dictionary with words to be removed from the corpus.</p>
<div class="sourceCode" id="cb505"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb505-1"><a href="text-mining-natural-language-processing.html#cb505-1" aria-hidden="true" tabindex="-1"></a>dictionary <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;berkshire&quot;</span>,<span class="st">&quot;hathaway&quot;</span>, <span class="st">&quot;million&quot;</span>, <span class="st">&quot;billion&quot;</span>, <span class="st">&quot;dollar&quot;</span>)</span>
<span id="cb505-2"><a href="text-mining-natural-language-processing.html#cb505-2" aria-hidden="true" tabindex="-1"></a>clean.letters <span class="ot">&lt;-</span> <span class="fu">tm_map</span>(clean.letters,removeWords,dictionary)</span></code></pre></div>
</div>
<div id="word-length-filtering" class="section level3 unnumbered">
<h3>Word length filtering</h3>
<p>You can also apply a filter to remove all words less than or greater
than a specified lengths. The tm package provides this option when
generating a term frequency matrix, something you will read about
shortly.</p>
</div>
<div id="parts-of-speech-pos-filtering" class="section level3 unnumbered">
<h3>Parts of speech (POS) filtering</h3>
<p>Another option is to remove particular types of words. For example, you
might scrub all adverbs and adjectives.</p>
</div>
<div id="stemming" class="section level3 unnumbered">
<h3>Stemming</h3>
<p>Stemming reduces inflected or derived words to their stem or root form.
For example, <em>cats</em> and <em>catty</em> stem to <em>cat</em>. <em>Fishlike</em> and <em>fishy</em>
stem to <em>fish</em>. As a stemmer generally works by suffix stripping, so
<em>catfish</em> should stem to <em>cat</em>. As you would expect, stemmers are
available for different languages, and thus the language must be
specified.</p>
<div class="sourceCode" id="cb506"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb506-1"><a href="text-mining-natural-language-processing.html#cb506-1" aria-hidden="true" tabindex="-1"></a><span class="co"># stem the document -- might take a while to run</span></span>
<span id="cb506-2"><a href="text-mining-natural-language-processing.html#cb506-2" aria-hidden="true" tabindex="-1"></a>stem.letters <span class="ot">&lt;-</span> <span class="fu">tm_map</span>(clean.letters,stemDocument, <span class="at">language =</span></span>
<span id="cb506-3"><a href="text-mining-natural-language-processing.html#cb506-3" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;english&quot;</span>)</span></code></pre></div>
<p>Following stemming, you can apply <strong>stem completion</strong> to return stems to
their original form to make the text more readable. The original
document that was stemmed, in this case, is used as the dictionary to
search for possible completions. Stem completion can apply several
different rules for converting a stem to a word, including “prevalent”
for the most frequent match, “first” for the first found completion, and
“longest” and “shortest” for the longest and shortest, respectively,
completion in terms of characters</p>
<div class="sourceCode" id="cb507"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb507-1"><a href="text-mining-natural-language-processing.html#cb507-1" aria-hidden="true" tabindex="-1"></a><span class="co"># stem completion -- might take a while to run</span></span>
<span id="cb507-2"><a href="text-mining-natural-language-processing.html#cb507-2" aria-hidden="true" tabindex="-1"></a>stem.letters <span class="ot">&lt;-</span>  <span class="fu">tm_map</span>(clean.letters,stemDocument, <span class="at">language =</span> <span class="st">&quot;english&quot;</span>)</span></code></pre></div>
</div>
<div id="regex-filtering" class="section level3 unnumbered">
<h3>Regex filtering</h3>
<p>The power of regex (regular expressions) can also be used for filtering
text or searching and replacing text. You might recall that we covered
regex when learning SQL.</p>
</div>
</div>
<div id="word-frequency-analysis" class="section level2 unnumbered">
<h2>Word frequency analysis</h2>
<p>Word frequency analysis is a simple technique that can also be the
foundation for other analyses. The method is based on creating a matrix
in one of two forms.</p>
<p>A <strong>term-document matrix</strong> contains one row for each term and one column
for each document.</p>
<div class="sourceCode" id="cb508"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb508-1"><a href="text-mining-natural-language-processing.html#cb508-1" aria-hidden="true" tabindex="-1"></a>tdm <span class="ot">&lt;-</span>  <span class="fu">TermDocumentMatrix</span>(stem.letters,<span class="at">control =</span> <span class="fu">list</span>(<span class="at">minWordLength=</span><span class="dv">3</span>))</span>
<span id="cb508-2"><a href="text-mining-natural-language-processing.html#cb508-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(tdm)</span></code></pre></div>
<pre><code>## [1] 6966   15</code></pre>
<div class="sourceCode" id="cb510"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb510-1"><a href="text-mining-natural-language-processing.html#cb510-1" aria-hidden="true" tabindex="-1"></a><span class="co"># report those words occurring more than 100 times</span></span>
<span id="cb510-2"><a href="text-mining-natural-language-processing.html#cb510-2" aria-hidden="true" tabindex="-1"></a><span class="fu">findFreqTerms</span>(tdm, <span class="at">lowfreq =</span> <span class="dv">100</span>, <span class="at">highfreq =</span> <span class="cn">Inf</span>)</span></code></pre></div>
<pre><code>##   [1] &quot;account&quot;     &quot;acquisit&quot;    &quot;addit&quot;       &quot;ago&quot;         &quot;american&quot;   
##   [6] &quot;amount&quot;      &quot;annual&quot;      &quot;asset&quot;       &quot;berkshir&quot;    &quot;bond&quot;       
##  [11] &quot;book&quot;        &quot;busi&quot;        &quot;buy&quot;         &quot;call&quot;        &quot;capit&quot;      
##  [16] &quot;case&quot;        &quot;cash&quot;        &quot;ceo&quot;         &quot;charg&quot;       &quot;charli&quot;     
##  [21] &quot;compani&quot;     &quot;continu&quot;     &quot;corpor&quot;      &quot;cost&quot;        &quot;countri&quot;    
##  [26] &quot;custom&quot;      &quot;day&quot;         &quot;deliv&quot;       &quot;director&quot;    &quot;earn&quot;       
##  [31] &quot;econom&quot;      &quot;expect&quot;      &quot;expens&quot;      &quot;famili&quot;      &quot;financi&quot;    
##  [36] &quot;float&quot;       &quot;fund&quot;        &quot;futur&quot;       &quot;gain&quot;        &quot;geico&quot;      
##  [41] &quot;general&quot;     &quot;give&quot;        &quot;good&quot;        &quot;great&quot;       &quot;group&quot;      
##  [46] &quot;growth&quot;      &quot;hold&quot;        &quot;huge&quot;        &quot;import&quot;      &quot;includ&quot;     
##  [51] &quot;incom&quot;       &quot;increas&quot;     &quot;industri&quot;    &quot;insur&quot;       &quot;interest&quot;   
##  [56] &quot;intrins&quot;     &quot;invest&quot;      &quot;investor&quot;    &quot;job&quot;         &quot;larg&quot;       
##  [61] &quot;largest&quot;     &quot;long&quot;        &quot;loss&quot;        &quot;made&quot;        &quot;major&quot;      
##  [66] &quot;make&quot;        &quot;manag&quot;       &quot;market&quot;      &quot;meet&quot;        &quot;money&quot;      
##  [71] &quot;net&quot;         &quot;number&quot;      &quot;offer&quot;       &quot;open&quot;        &quot;oper&quot;       
##  [76] &quot;own&quot;         &quot;owner&quot;       &quot;page&quot;        &quot;paid&quot;        &quot;pay&quot;        
##  [81] &quot;peopl&quot;       &quot;perform&quot;     &quot;period&quot;      &quot;point&quot;       &quot;polici&quot;     
##  [86] &quot;posit&quot;       &quot;premium&quot;     &quot;present&quot;     &quot;price&quot;       &quot;problem&quot;    
##  [91] &quot;produc&quot;      &quot;product&quot;     &quot;profit&quot;      &quot;purchas&quot;     &quot;put&quot;        
##  [96] &quot;question&quot;    &quot;rate&quot;        &quot;receiv&quot;      &quot;record&quot;      &quot;reinsur&quot;    
## [101] &quot;remain&quot;      &quot;report&quot;      &quot;requir&quot;      &quot;reserv&quot;      &quot;result&quot;     
## [106] &quot;return&quot;      &quot;run&quot;         &quot;sale&quot;        &quot;saturday&quot;    &quot;sell&quot;       
## [111] &quot;servic&quot;      &quot;share&quot;       &quot;sharehold&quot;   &quot;signific&quot;    &quot;special&quot;    
## [116] &quot;stock&quot;       &quot;sunday&quot;      &quot;tax&quot;         &quot;time&quot;        &quot;today&quot;      
## [121] &quot;total&quot;       &quot;underwrit&quot;   &quot;work&quot;        &quot;world&quot;       &quot;worth&quot;      
## [126] &quot;year&quot;        &quot;—&quot;           &quot;contract&quot;    &quot;home&quot;        &quot;midamerican&quot;
## [131] &quot;risk&quot;        &quot;deriv&quot;       &quot;clayton&quot;</code></pre>
<p>A <strong>document-term matrix</strong> contains one row for each document and one
column for each term.</p>
<div class="sourceCode" id="cb512"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb512-1"><a href="text-mining-natural-language-processing.html#cb512-1" aria-hidden="true" tabindex="-1"></a>dtm <span class="ot">&lt;-</span> <span class="fu">DocumentTermMatrix</span>(stem.letters,<span class="at">control =</span> <span class="fu">list</span>(<span class="at">minWordLength=</span><span class="dv">3</span>))</span>
<span id="cb512-2"><a href="text-mining-natural-language-processing.html#cb512-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(dtm)</span></code></pre></div>
<pre><code>## [1]   15 6966</code></pre>
<p>The function dtm() reports the number of distinct terms, the vocabulary,
and the number of documents in the corpus.</p>
<div id="term-frequency" class="section level3 unnumbered">
<h3>Term frequency</h3>
<p>Words that occur frequently within a document are usually a good
indicator of the document’s content. Term frequency (tf) measures word
frequency.</p>
<p>tf<sub>td</sub> = number of times term t occurs in document d.</p>
<p>Here is the R code for determining the frequency of words in a corpus.</p>
<div class="sourceCode" id="cb514"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb514-1"><a href="text-mining-natural-language-processing.html#cb514-1" aria-hidden="true" tabindex="-1"></a>tdm <span class="ot">&lt;-</span>  <span class="fu">TermDocumentMatrix</span>(stem.letters,<span class="at">control =</span> <span class="fu">list</span>(<span class="at">minWordLength=</span><span class="dv">3</span>))</span>
<span id="cb514-2"><a href="text-mining-natural-language-processing.html#cb514-2" aria-hidden="true" tabindex="-1"></a><span class="co"># convert term document matrix to a regular matrix to get frequencies of words</span></span>
<span id="cb514-3"><a href="text-mining-natural-language-processing.html#cb514-3" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span>  <span class="fu">as.matrix</span>(tdm)</span>
<span id="cb514-4"><a href="text-mining-natural-language-processing.html#cb514-4" aria-hidden="true" tabindex="-1"></a><span class="co"># sort on frequency of terms</span></span>
<span id="cb514-5"><a href="text-mining-natural-language-processing.html#cb514-5" aria-hidden="true" tabindex="-1"></a>v <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">rowSums</span>(m), <span class="at">decreasing=</span><span class="cn">TRUE</span>)</span>
<span id="cb514-6"><a href="text-mining-natural-language-processing.html#cb514-6" aria-hidden="true" tabindex="-1"></a><span class="co"># display the ten most frequent words</span></span>
<span id="cb514-7"><a href="text-mining-natural-language-processing.html#cb514-7" aria-hidden="true" tabindex="-1"></a>v[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]</span></code></pre></div>
<pre><code>##      year      busi   compani      earn      oper     insur     manag    invest 
##      1288       969       709       665       555       547       476       405 
##      make sharehold 
##       353       348</code></pre>
<p>A probability density plot shows the distribution of words in a document
visually. As you can see, there is a very long and thin tail because a
very small number of words occur frequently. Note that this plot shows
the distribution of words after the removal of stop words.</p>
<div class="sourceCode" id="cb516"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb516-1"><a href="text-mining-natural-language-processing.html#cb516-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb516-2"><a href="text-mining-natural-language-processing.html#cb516-2" aria-hidden="true" tabindex="-1"></a><span class="co"># get the names corresponding to the words</span></span>
<span id="cb516-3"><a href="text-mining-natural-language-processing.html#cb516-3" aria-hidden="true" tabindex="-1"></a>names <span class="ot">&lt;-</span> <span class="fu">names</span>(v)</span>
<span id="cb516-4"><a href="text-mining-natural-language-processing.html#cb516-4" aria-hidden="true" tabindex="-1"></a><span class="co"># create a data frame for plotting</span></span>
<span id="cb516-5"><a href="text-mining-natural-language-processing.html#cb516-5" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">word=</span>names, <span class="at">freq=</span>v)</span>
<span id="cb516-6"><a href="text-mining-natural-language-processing.html#cb516-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(d,<span class="fu">aes</span>(freq)) <span class="sc">+</span> </span>
<span id="cb516-7"><a href="text-mining-natural-language-processing.html#cb516-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">fill=</span><span class="st">&quot;salmon&quot;</span>) <span class="sc">+</span> </span>
<span id="cb516-8"><a href="text-mining-natural-language-processing.html#cb516-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Frequency&quot;</span>) <span class="sc">+</span></span>
<span id="cb516-9"><a href="text-mining-natural-language-processing.html#cb516-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Density&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:density-plot"></span>
<img src="DataManagement_files/figure-html/density-plot-1.png" alt="Probability density plot of word frequency" width="672" />
<p class="caption">
Figure 17.1: Probability density plot of word frequency
</p>
</div>
<p>A word cloud is way of visualizing the most frequent words.</p>
<div class="sourceCode" id="cb517"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb517-1"><a href="text-mining-natural-language-processing.html#cb517-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(wordcloud)</span>
<span id="cb517-2"><a href="text-mining-natural-language-processing.html#cb517-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RColorBrewer)</span>
<span id="cb517-3"><a href="text-mining-natural-language-processing.html#cb517-3" aria-hidden="true" tabindex="-1"></a><span class="co"># select the color palette</span></span>
<span id="cb517-4"><a href="text-mining-natural-language-processing.html#cb517-4" aria-hidden="true" tabindex="-1"></a>pal <span class="ot">=</span> <span class="fu">brewer.pal</span>(<span class="dv">5</span>,<span class="st">&quot;Accent&quot;</span>)</span>
<span id="cb517-5"><a href="text-mining-natural-language-processing.html#cb517-5" aria-hidden="true" tabindex="-1"></a><span class="co"># generate the cloud based on the 30 most frequent words</span></span>
<span id="cb517-6"><a href="text-mining-natural-language-processing.html#cb517-6" aria-hidden="true" tabindex="-1"></a><span class="fu">wordcloud</span>(d<span class="sc">$</span>word, d<span class="sc">$</span>freq, <span class="at">min.freq=</span>d<span class="sc">$</span>freq[<span class="dv">30</span>],<span class="at">colors=</span>pal)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:cloud"></span>
<img src="DataManagement_files/figure-html/cloud-1.png" alt="A word cloud" width="672" />
<p class="caption">
Figure 17.2: A word cloud
</p>
</div>
<blockquote>
<p>❓<strong>Skill builder</strong></p>
<p>Start with the original letters corpus (i.e., prior to preprocessing)
and identify the 20 most common words and create a word cloud for
these words.</p>
</blockquote>
</div>
</div>
<div id="co-occurrence-and-association" class="section level2 unnumbered">
<h2>Co-occurrence and association</h2>
<p>Co-occurrence measures the frequency with which two words appear
together. In the case of document level association, if the two words
both appear or neither appears, then the correlation or association is
1. If two words never appear together in the same document, their
association is -1.</p>
<p>A simple example illustrates the concept. The following code sets up a
corpus of five elementary documents.</p>
<div class="sourceCode" id="cb518"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb518-1"><a href="text-mining-natural-language-processing.html#cb518-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tm)</span>
<span id="cb518-2"><a href="text-mining-natural-language-processing.html#cb518-2" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span>  <span class="fu">c</span>(<span class="st">&quot;word1&quot;</span>, <span class="st">&quot;word1 word2&quot;</span>,<span class="st">&quot;word1 word2 word3&quot;</span>,<span class="st">&quot;word1 word2 word3 word4&quot;</span>,<span class="st">&quot;word1 word2 word3 word4 word5&quot;</span>)</span>
<span id="cb518-3"><a href="text-mining-natural-language-processing.html#cb518-3" aria-hidden="true" tabindex="-1"></a>corpus <span class="ot">&lt;-</span>  <span class="fu">VCorpus</span>(<span class="fu">VectorSource</span>(data))</span>
<span id="cb518-4"><a href="text-mining-natural-language-processing.html#cb518-4" aria-hidden="true" tabindex="-1"></a>tdm <span class="ot">&lt;-</span>  <span class="fu">TermDocumentMatrix</span>(corpus)</span>
<span id="cb518-5"><a href="text-mining-natural-language-processing.html#cb518-5" aria-hidden="true" tabindex="-1"></a><span class="fu">as.matrix</span>(tdm)</span></code></pre></div>
<pre><code>##        Docs
## Terms   1 2 3 4 5
##   word1 1 1 1 1 1
##   word2 0 1 1 1 1
##   word3 0 0 1 1 1
##   word4 0 0 0 1 1
##   word5 0 0 0 0 1</code></pre>
<p>We compute the correlation of rows to get a measure of association
across documents.</p>
<div class="sourceCode" id="cb520"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb520-1"><a href="text-mining-natural-language-processing.html#cb520-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Correlation between word2 and word3, word4, and word5</span></span>
<span id="cb520-2"><a href="text-mining-natural-language-processing.html#cb520-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>),<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
<pre><code>## [1] 0.6123724</code></pre>
<div class="sourceCode" id="cb522"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb522-1"><a href="text-mining-natural-language-processing.html#cb522-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>),<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
<pre><code>## [1] 0.4082483</code></pre>
<div class="sourceCode" id="cb524"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb524-1"><a href="text-mining-natural-language-processing.html#cb524-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>),<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>))</span></code></pre></div>
<pre><code>## [1] 0.25</code></pre>
<p>Alternatively, use the findAssocs function, which computes all
correlations between a given term and all terms in the term-document
matrix and reports those higher than the correlation threshold.</p>
<div class="sourceCode" id="cb526"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb526-1"><a href="text-mining-natural-language-processing.html#cb526-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tm)</span>
<span id="cb526-2"><a href="text-mining-natural-language-processing.html#cb526-2" aria-hidden="true" tabindex="-1"></a><span class="co"># find associations greater than 0.1</span></span>
<span id="cb526-3"><a href="text-mining-natural-language-processing.html#cb526-3" aria-hidden="true" tabindex="-1"></a><span class="fu">findAssocs</span>(tdm,<span class="st">&quot;word2&quot;</span>,<span class="fl">0.1</span>)</span></code></pre></div>
<pre><code>## $word2
## word3 word4 word5 
##  0.61  0.41  0.25</code></pre>
<p>Now that you have an understanding of how association works across
documents, here is an example for the corpus of Buffett letters.</p>
<div class="sourceCode" id="cb528"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb528-1"><a href="text-mining-natural-language-processing.html#cb528-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Select the first ten letters</span></span>
<span id="cb528-2"><a href="text-mining-natural-language-processing.html#cb528-2" aria-hidden="true" tabindex="-1"></a>tdm <span class="ot">&lt;-</span> <span class="fu">TermDocumentMatrix</span>(stem.letters[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>])</span>
<span id="cb528-3"><a href="text-mining-natural-language-processing.html#cb528-3" aria-hidden="true" tabindex="-1"></a><span class="co"># compute the associations</span></span>
<span id="cb528-4"><a href="text-mining-natural-language-processing.html#cb528-4" aria-hidden="true" tabindex="-1"></a><span class="fu">findAssocs</span>(tdm, <span class="st">&quot;invest&quot;</span>,<span class="fl">0.80</span>)</span></code></pre></div>
<pre><code>## $invest
##      bare     earth imperfect     resum     susan 
##      0.88      0.88      0.88      0.84      0.84</code></pre>
</div>
<div id="cluster-analysis" class="section level2 unnumbered">
<h2>Cluster analysis</h2>
<p>Cluster analysis is a statistical technique for grouping together sets
of observations that share common characteristics. Objects assigned to
the same group are more similar in some way than those allocated to
another cluster. In the case of a corpus, cluster analysis groups
documents based on their similarity. Google, for instance, uses
clustering for its news site.</p>
<p>The general principle of cluster analysis is to map a set of
observations in multidimensional space. For example, if you have seven
measures for each observation, each will be mapped into
seven-dimensional space. Observations that are close together in this
space will be grouped together. In the case of a corpus, cluster
analysis is based on mapping frequently occurring words into a
multidimensional space. The frequency with which each word appears in a
document is used as a weight, so that frequently occurring words have
more influence than others.</p>
<p>There are multiple statistical techniques for clustering, and multiple
methods for calculating the distance between points. Furthermore, the
analyst has to decide how many clusters to create. Thus, cluster
analysis requires some judgment and experimentation to develop a
meaningful set of groups.</p>
<p>The following code computes all possible clusters using the Ward method
of cluster analysis. A term-document matrix is sparse, which means it
consists mainly of zeroes. In other words, many terms occur in only one
or two documents, and the cell entries for the remaining documents are
zero. In order to reduce the computations required, sparse terms are
removed from the matrix. You can vary the sparse parameter to see how
the clusters vary.</p>
<div class="sourceCode" id="cb530"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb530-1"><a href="text-mining-natural-language-processing.html#cb530-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Cluster analysis</span></span>
<span id="cb530-2"><a href="text-mining-natural-language-processing.html#cb530-2" aria-hidden="true" tabindex="-1"></a><span class="co"># name the columns for the letter&#39;s year</span></span>
<span id="cb530-3"><a href="text-mining-natural-language-processing.html#cb530-3" aria-hidden="true" tabindex="-1"></a>tdm <span class="ot">&lt;-</span> <span class="fu">TermDocumentMatrix</span>(stem.letters[<span class="dv">1</span><span class="sc">:</span><span class="dv">15</span>])</span>
<span id="cb530-4"><a href="text-mining-natural-language-processing.html#cb530-4" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(tdm) <span class="ot">&lt;-</span>  <span class="dv">1998</span><span class="sc">:</span><span class="dv">2012</span></span>
<span id="cb530-5"><a href="text-mining-natural-language-processing.html#cb530-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove sparse terms</span></span>
<span id="cb530-6"><a href="text-mining-natural-language-processing.html#cb530-6" aria-hidden="true" tabindex="-1"></a>tdm1 <span class="ot">&lt;-</span> <span class="fu">removeSparseTerms</span>(tdm, <span class="fl">0.5</span>) </span>
<span id="cb530-7"><a href="text-mining-natural-language-processing.html#cb530-7" aria-hidden="true" tabindex="-1"></a><span class="co"># transpose the matrix</span></span>
<span id="cb530-8"><a href="text-mining-natural-language-processing.html#cb530-8" aria-hidden="true" tabindex="-1"></a>tdmtranspose <span class="ot">&lt;-</span>  <span class="fu">t</span>(tdm1) </span>
<span id="cb530-9"><a href="text-mining-natural-language-processing.html#cb530-9" aria-hidden="true" tabindex="-1"></a>cluster <span class="ot">=</span> <span class="fu">hclust</span>(<span class="fu">dist</span>(tdmtranspose))</span>
<span id="cb530-10"><a href="text-mining-natural-language-processing.html#cb530-10" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the tree</span></span>
<span id="cb530-11"><a href="text-mining-natural-language-processing.html#cb530-11" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cluster)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:cluster"></span>
<img src="DataManagement_files/figure-html/cluster-1.png" alt="Dendrogram for Buffett letters from 1998-2012" width="672" />
<p class="caption">
Figure 17.3: Dendrogram for Buffett letters from 1998-2012
</p>
</div>
<p>The cluster analysis is shown in the following figure as a dendrogram, a
tree diagram, with a leaf for each year. Clusters seem to from around
consecutive years. Can you think of an explanation?</p>
</div>
<div id="topic-modeling" class="section level2 unnumbered">
<h2>Topic modeling</h2>
<p>Topic modeling is a set of statistical techniques for identifying the
themes that occur in a document set. The key assumption is that a
document on a particular topic will contain words that identify that
topic. For example, a report on gold mining will likely contain words
such as “gold” and “ore.” Whereas, a document on France, would likely
contain the terms “France,” “French,” and “Paris.”</p>
<p>The package <strong>topicmodels</strong> implements topic modeling techniques within
the R framework. It extends tm to provide support for topic modeling. It
implements two methods: latent Dirichlet allocation (LDA), which assumes
topics are uncorrelated; and correlated topic models (CTM), an extension
of LDA that permits correlation between topics.<a href="#fn60" class="footnote-ref" id="fnref60"><sup>60</sup></a> Both LDA
and CTM require that the number of topics to be extracted is determined
a priori. For example, you might decide in advance that five topics
gives a reasonable spread and is sufficiently small for the diversity to
be understood.<a href="#fn61" class="footnote-ref" id="fnref61"><sup>61</sup></a></p>
<p>Words that occur frequently in many documents are not good at
distinguishing among documents. The weighted term frequency inverse
document frequency (tf-idf) is a measure designed for determining which
terms discriminate among documents. It is based on the term frequency
(tf), defined earlier, and the inverse document frequency.</p>
<div id="inverse-document-frequency" class="section level3 unnumbered">
<h3>Inverse document frequency</h3>
<p>The inverse document frequency (idf) measures the frequency of a term
across documents.</p>
<p><img src="Figures/Chapter%2017/idf.png" /></p>
<blockquote>
<p>Where</p>
<p>m = number of documents (i.e., rows in the case of a term-document
matrix);</p>
<p>df<sub>t</sub> = number of documents containing term <em>t.</em></p>
</blockquote>
<p>If a term occurs in every document, then its idf = 0, whereas if a term
occurs in only one document out of 15, its idf = 3.91.</p>
<p>To calculate and display the idf for the letters corpus, we can use the
following R script.</p>
<div class="sourceCode" id="cb531"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb531-1"><a href="text-mining-natural-language-processing.html#cb531-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate idf for each term</span></span>
<span id="cb531-2"><a href="text-mining-natural-language-processing.html#cb531-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(slam)</span>
<span id="cb531-3"><a href="text-mining-natural-language-processing.html#cb531-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb531-4"><a href="text-mining-natural-language-processing.html#cb531-4" aria-hidden="true" tabindex="-1"></a>idf <span class="ot">&lt;-</span>  <span class="fu">log2</span>(<span class="fu">nDocs</span>(dtm)<span class="sc">/</span><span class="fu">col_sums</span>(dtm <span class="sc">&gt;</span> <span class="dv">0</span>))</span>
<span id="cb531-5"><a href="text-mining-natural-language-processing.html#cb531-5" aria-hidden="true" tabindex="-1"></a><span class="co"># create dataframe for graphing</span></span>
<span id="cb531-6"><a href="text-mining-natural-language-processing.html#cb531-6" aria-hidden="true" tabindex="-1"></a>df.idf <span class="ot">&lt;-</span>  <span class="fu">data.frame</span>(idf)</span>
<span id="cb531-7"><a href="text-mining-natural-language-processing.html#cb531-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df.idf,<span class="fu">aes</span>(idf)) <span class="sc">+</span> <span class="fu">geom_histogram</span>(<span class="at">fill=</span><span class="st">&quot;chocolate&quot;</span>) <span class="sc">+</span> <span class="fu">xlab</span>(<span class="st">&quot;Inverse document frequency&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:topic-1"></span>
<img src="DataManagement_files/figure-html/topic-1-1.png" alt="Inverse document frequency (corpus has 15 documents)" width="672" />
<p class="caption">
Figure 17.4: Inverse document frequency (corpus has 15 documents)
</p>
</div>
<p>The preceding graphic shows that about 5,000 terms occur in only one
document (i.e., the idf = 3.91) and less than 500 terms occur in every
document. The terms with an idf in the range 1 to 2 are likely to be the
most useful in determining the topic of each document.</p>
</div>
<div id="term-frequency-inverse-document-frequency-tf-idf" class="section level3 unnumbered">
<h3>Term frequency inverse document frequency (tf-idf)</h3>
<p>The weighted term frequency inverse document frequency (tf-idf or ω<sub>td</sub>)
is calculated by multiplying a term’s frequency (tf) by its inverse
document frequency (idf).</p>
<p><img src="Figures/Chapter%2017/tf-idf.png" /></p>
<p>Where</p>
<p>tf<sub>td</sub> = frequency of term <em>t</em> in document <em>d.</em></p>
</div>
<div id="topic-modeling-with-r" class="section level3 unnumbered">
<h3>Topic modeling with R</h3>
<p>Prior to topic modeling, pre-process a text file in the usual fashion
(e.g., convert to lower case, remove punctuation, and so forth). Then,
create a document-term matrix.</p>
<p>The mean term frequency-inverse document frequency (tf-idf) is used to
select the vocabulary for topic modeling. We use the median value of
tf-idf for all terms as a threshold.</p>
<div class="sourceCode" id="cb532"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb532-1"><a href="text-mining-natural-language-processing.html#cb532-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(topicmodels)</span>
<span id="cb532-2"><a href="text-mining-natural-language-processing.html#cb532-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(slam)</span>
<span id="cb532-3"><a href="text-mining-natural-language-processing.html#cb532-3" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate tf-idf for each term</span></span>
<span id="cb532-4"><a href="text-mining-natural-language-processing.html#cb532-4" aria-hidden="true" tabindex="-1"></a>tfidf <span class="ot">&lt;-</span>  <span class="fu">tapply</span>(dtm<span class="sc">$</span>v<span class="sc">/</span><span class="fu">row_sums</span>(dtm)[dtm<span class="sc">$</span>i], dtm<span class="sc">$</span>j, mean) <span class="sc">*</span> <span class="fu">log2</span>(<span class="fu">nDocs</span>(dtm)<span class="sc">/</span><span class="fu">col_sums</span>(dtm <span class="sc">&gt;</span> <span class="dv">0</span>))</span>
<span id="cb532-5"><a href="text-mining-natural-language-processing.html#cb532-5" aria-hidden="true" tabindex="-1"></a><span class="co"># report dimensions (terms)</span></span>
<span id="cb532-6"><a href="text-mining-natural-language-processing.html#cb532-6" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(tfidf)</span></code></pre></div>
<pre><code>## [1] 6966</code></pre>
<div class="sourceCode" id="cb534"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb534-1"><a href="text-mining-natural-language-processing.html#cb534-1" aria-hidden="true" tabindex="-1"></a><span class="co"># report median to use as cut-off point</span></span>
<span id="cb534-2"><a href="text-mining-natural-language-processing.html#cb534-2" aria-hidden="true" tabindex="-1"></a><span class="fu">median</span>(tfidf)</span></code></pre></div>
<pre><code>## [1] 0.0006601708</code></pre>
<p>The goal of topic modeling is to find those terms that distinguish a
document set. Thus, terms with low frequency should be omitted because
they don’t occur often enough to define a topic. Similarly, those terms
occurring in many documents don’t differentiate between documents.</p>
<p>A common heuristic is to select those terms with a tf-idf &gt;
median(tf-idf). As a result, we reduce the document-term matrix by
keeping only those terms above the threshold and eliminating rows that
have zero terms. Because the median is a measure of central tendency,
this approach reduces the number of columns by roughly half.</p>
<div class="sourceCode" id="cb536"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb536-1"><a href="text-mining-natural-language-processing.html#cb536-1" aria-hidden="true" tabindex="-1"></a><span class="co"># select columns with tf-idf &gt; median</span></span>
<span id="cb536-2"><a href="text-mining-natural-language-processing.html#cb536-2" aria-hidden="true" tabindex="-1"></a>dtm <span class="ot">&lt;-</span> dtm[,tfidf <span class="sc">&gt;=</span> <span class="fu">median</span>(tfidf)]</span>
<span id="cb536-3"><a href="text-mining-natural-language-processing.html#cb536-3" aria-hidden="true" tabindex="-1"></a><span class="co">#select rows with rowsum &gt; 0</span></span>
<span id="cb536-4"><a href="text-mining-natural-language-processing.html#cb536-4" aria-hidden="true" tabindex="-1"></a>dtm <span class="ot">&lt;-</span> dtm[<span class="fu">row_sums</span>(dtm) <span class="sc">&gt;</span> <span class="dv">0</span>,]</span>
<span id="cb536-5"><a href="text-mining-natural-language-processing.html#cb536-5" aria-hidden="true" tabindex="-1"></a><span class="co"># report reduced dimension</span></span>
<span id="cb536-6"><a href="text-mining-natural-language-processing.html#cb536-6" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(dtm)</span></code></pre></div>
<pre><code>## [1]   15 3509</code></pre>
<p>As mentioned earlier, the topic modeling method assumes a set number of
topics, and it is the responsibility of the analyst to estimate the
correct number of topics to extract. It is common practice to fit models
with a varying number of topics, and use the various results to
establish a good choice for the number of topics. The analyst will
typically review the output of several models and make a judgment on
which model appears to provide a realistic set of distinct topics. Here
is some code that starts with five topics.</p>
<div class="sourceCode" id="cb538"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb538-1"><a href="text-mining-natural-language-processing.html#cb538-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set number of topics to extract</span></span>
<span id="cb538-2"><a href="text-mining-natural-language-processing.html#cb538-2" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">5</span> <span class="co"># number of topics </span></span>
<span id="cb538-3"><a href="text-mining-natural-language-processing.html#cb538-3" aria-hidden="true" tabindex="-1"></a>SEED <span class="ot">&lt;-</span> <span class="dv">2010</span> <span class="co"># seed for initializing the model rather than the default random</span></span>
<span id="cb538-4"><a href="text-mining-natural-language-processing.html#cb538-4" aria-hidden="true" tabindex="-1"></a><span class="co"># try multiple methods – takes a while for a big corpus</span></span>
<span id="cb538-5"><a href="text-mining-natural-language-processing.html#cb538-5" aria-hidden="true" tabindex="-1"></a>TM <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">VEM =</span> <span class="fu">LDA</span>(dtm, <span class="at">k =</span> k, <span class="at">control =</span> <span class="fu">list</span>(<span class="at">seed =</span> SEED)),  </span>
<span id="cb538-6"><a href="text-mining-natural-language-processing.html#cb538-6" aria-hidden="true" tabindex="-1"></a><span class="at">VEM_fixed =</span> <span class="fu">LDA</span>(dtm, <span class="at">k =</span> k, <span class="at">control =</span> <span class="fu">list</span>(<span class="at">estimate.alpha =</span> <span class="cn">FALSE</span>, <span class="at">seed =</span> SEED)), </span>
<span id="cb538-7"><a href="text-mining-natural-language-processing.html#cb538-7" aria-hidden="true" tabindex="-1"></a><span class="at">Gibbs =</span> <span class="fu">LDA</span>(dtm, <span class="at">k =</span> k, <span class="at">method =</span> <span class="st">&quot;Gibbs&quot;</span>, <span class="at">control =</span> <span class="fu">list</span>(<span class="at">seed =</span> SEED, <span class="at">burnin =</span> <span class="dv">1000</span>,  <span class="at">thin =</span> <span class="dv">100</span>, <span class="at">iter =</span> <span class="dv">1000</span>)), <span class="at">CTM =</span> <span class="fu">CTM</span>(dtm, <span class="at">k =</span> k,<span class="at">control =</span> <span class="fu">list</span>(<span class="at">seed =</span> SEED, <span class="at">var =</span> <span class="fu">list</span>(<span class="at">tol =</span> <span class="dv">10</span><span class="sc">^-</span><span class="dv">3</span>), <span class="at">em =</span> <span class="fu">list</span>(<span class="at">tol =</span> <span class="dv">10</span><span class="sc">^-</span><span class="dv">3</span>))))</span>
<span id="cb538-8"><a href="text-mining-natural-language-processing.html#cb538-8" aria-hidden="true" tabindex="-1"></a><span class="fu">topics</span>(TM[[<span class="st">&quot;VEM&quot;</span>]], <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 
##    2    2    2    3    1    3    3    3    4    3    5    4    1    1    5</code></pre>
<div class="sourceCode" id="cb540"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb540-1"><a href="text-mining-natural-language-processing.html#cb540-1" aria-hidden="true" tabindex="-1"></a><span class="fu">terms</span>(TM[[<span class="st">&quot;VEM&quot;</span>]], <span class="dv">5</span>)</span></code></pre></div>
<pre><code>##      Topic 1     Topic 2     Topic 3   Topic 4   Topic 5     
## [1,] &quot;’&quot;         &quot;—&quot;         &quot;clayton&quot; &quot;walter&quot;  &quot;newspap&quot;   
## [2,] &quot;repurchas&quot; &quot;eja&quot;       &quot;deficit&quot; &quot;clayton&quot; &quot;clayton&quot;   
## [3,] &quot;committe&quot;  &quot;repurchas&quot; &quot;see&quot;     &quot;newspap&quot; &quot;repurchas&quot; 
## [4,] &quot;economi&quot;   &quot;merger&quot;    &quot;member&quot;  &quot;iscar&quot;   &quot;journalist&quot;
## [5,] &quot;bnsf&quot;      &quot;ticket&quot;    &quot;—&quot;       &quot;equita&quot;  &quot;monolin&quot;</code></pre>
<p>The output indicates that the first three letter (1998-2000) are about
topic 4, the fourth (2001) topic 2, and so on.</p>
<p>Topic 1 is defined by the following terms: thats, bnsf, cant,
blacksholes, and railroad. As we have seen previously, some of these
words (e.g., thats and cant, which we can infer as being that’s and
can’t) are not useful differentiators, and the dictionary could be
extended to remove them from consideration and topic modeling repeated.
For this particular case, it might be that Buffett’s letters don’t vary
much from year to year, and he returns to the same topics in each annual
report.</p>
<blockquote>
<p>❓<strong>Skill builder</strong></p>
<p>Experiment with the topicmodels package to identify the topics in
Buffett’s letters. You might need to use the dictionary feature of
text mining to remove selected words from the corpus to develop a
meaningful distinction between topics.</p>
</blockquote>
</div>
</div>
<div id="named-entity-recognition-ner" class="section level2 unnumbered">
<h2>Named-entity recognition (NER)</h2>
<p>Named-entity recognition (NER) places terms in a corpus into predefined
categories such as the names of persons, organizations, locations,
expressions of times, quantities, monetary values, and percentages. It
identifies some or all mentions of these categories, as shown in the
following figure, where an organization, place, and date are recognized.</p>
<p>Named-entity recognition example</p>
<p><img src="Figures/Chapter%2017/name-entity-recognition.png" /></p>
<p>Tags are added to the corpus to denote the category of the terms
identified.</p>
<p>The &lt;organization&gt;Olympics&lt;/organization&gt; were in
&lt;place&gt;London&lt;/place&gt; in &lt;date&gt;2012&lt;/date&gt;.</p>
<p>There are two approaches to developing an NER capability. A
<em>rules-based</em> approach works well for a well-understood domain, but it
requires maintenance and is language dependent. <em>Statistical
classifiers</em>, based on machine learning, look at each word in a sentence
to decide whether it is the start of a named-entity, a continuation of
an already identified named-entity, or not part of a named-entity. Once
a named-entity is distinguished, its category (e.g., place) must be
identified and surrounding tags inserted.</p>
<p>Statistical classifiers need to be trained on a large collection of
human-annotated text that can be used as input to machine learning
software. Human-annotation, while time-consuming, does not require a
high level of skill. It is the sort of task that is easily parallelized
and distributed to a large number of human coders who have a reasonable
understanding of the corpus’s context (e.g., able to recognize that
London is a place and that the Olympics is an organization). The
software classifier need to be trained on approximately 30,000 words.</p>
<p>The accuracy of NER is dependent on the corpus used for training and the
domain of the documents to be classified. For example, NER is based on a
collection of news stories and is unlikely to be very accurate for
recognizing entities in medical or scientific literature. Thus, for some
domains, you will likely need to annotate a set of sample documents to
create a relevant model. Of course, as times change, it might be
necessary to add new annotated text to the learning script to
accommodate new organizations, place, people and so forth. A
well-trained statistical classifier applied appropriately is usually
capable of correctly recognizing entities with 90 percent accuracy.</p>
<div id="ner-software" class="section level3 unnumbered">
<h3>NER software</h3>
<p>OpenNLP<a href="#fn62" class="footnote-ref" id="fnref62"><sup>62</sup></a> is an Apache Java-based machine learning based
toolkit for the processing of natural language in text format. It is a
collection of natural language processing tools, including a sentence
detector, tokenizer, parts-of-speech(POS)-tagger, syntactic parser, and
named-entity detector. The NER tool can recognize people, locations,
organizations, dates, times. percentages, and money. You will need to
write a Java program to take advantage of the toolkit. The R package,
openNLP, provides an interface to OpenNLP.</p>
</div>
</div>
<div id="future-developments" class="section level2 unnumbered">
<h2>Future developments</h2>
<p>Text mining and natural language processing are developing areas and you
can expect new tools to emerge. Document summarization, relationship
extraction, advanced sentiment analysis, and cross-language information
retrieval (e.g., a Chinese speaker querying English documents and
getting a Chinese translation of the search and selected documents) are
all areas of research that will likely result in generally available
software with possible R versions. If you work in this area, you will
need to continually scan for new software that extends the power of
existing methods and adds new text mining capabilities.</p>
</div>
<div id="summary-17" class="section level2 unnumbered">
<h2>Summary</h2>
<p>Language enables cooperation through information exchange. Natural
language processing (NLP) focuses on developing and implementing
software that enables computers to handle large scale processing of
language in a variety of forms, such as written and spoken. The inherent
ambiguity in written and spoken speech makes NLP challenging. Don’t
expect NLP to provide the same level of exactness and starkness as
numeric processing. There are three levels to consider when processing
language: semantics, discourse, and pragmatics.</p>
<p>Sentiment analysis is a popular and simple method of measuring aggregate
feeling. Tokenization is the process of breaking a document into chunks.
A collection of text is called a corpus. The Flesch-Kincaid formula is a
common way of assessing readability. Preprocessing, which prepares a
corpus for text mining, can include case conversion, punctuation
removal, number removal, stripping extra white spaces, stop word
filtering, specific word removal, word length filtering, parts of speech
(POS) filtering, Stemming, and regex filtering.</p>
<p>Word frequency analysis is a simple technique that can also be the
foundation for other analyses. A term-document matrix contains one row
for each term and one column for each document. A document-term matrix
contains one row for each document and one column for each term. Words
that occur frequently within a document are usually a good indicator of
the document’s content. A word cloud is way of visualizing the most
frequent words. Co-occurrence measures the frequency with which two
words appear together. Cluster analysis is a statistical technique for
grouping together sets of observations that share common
characteristics. Topic modeling is a set of statistical techniques for
identifying the topics that occur in a document set. The inverse
document frequency (idf) measures the frequency of a term across
documents. Named-entity recognition (NER) places terms in a corpus into
predefined categories such as the names of persons, organizations,
locations, expressions of times, quantities, monetary values, and
percentages. Statistical classification is used for NER. OpenNLP is an
Apache Java-based machine learning-based toolkit for the processing of
natural language in text format. Document summarization, relationship
extraction, advanced sentiment analysis, and cross-language information
retrieval are all areas of research.</p>
</div>
<div id="key-terms-and-concepts-13" class="section level2 unnumbered">
<h2>Key terms and concepts</h2>
<table>
<tbody>
<tr class="odd">
<td>Association</td>
<td>Readability</td>
</tr>
<tr class="even">
<td>Cluster analysis</td>
<td>Regex filtering</td>
</tr>
<tr class="odd">
<td>Co-occurrence</td>
<td>Sentiment analysis</td>
</tr>
<tr class="even">
<td>Corpus</td>
<td>Statistical classification</td>
</tr>
<tr class="odd">
<td>Dendrogram</td>
<td>Stemming</td>
</tr>
<tr class="even">
<td>Document-term matrix</td>
<td>Stop word filtering</td>
</tr>
<tr class="odd">
<td>Flesch-Kincaid formula</td>
<td>Stripping extra white spaces</td>
</tr>
<tr class="even">
<td>Inverse document frequency</td>
<td>Term-document matrix</td>
</tr>
<tr class="odd">
<td>KNIME</td>
<td>Term frequency</td>
</tr>
<tr class="even">
<td>Named-entity recognition (NER)</td>
<td>Term frequency inverse document frequency</td>
</tr>
<tr class="odd">
<td>Natural language processing (NLP)</td>
<td>Text mining</td>
</tr>
<tr class="even">
<td>Number removal</td>
<td>Tokenization</td>
</tr>
<tr class="odd">
<td>OpenNLP</td>
<td>Topic modeling</td>
</tr>
<tr class="even">
<td>Parts of speech (POS) filtering</td>
<td>Word cloud</td>
</tr>
<tr class="odd">
<td>Preprocessing</td>
<td>Word frequency analysis</td>
</tr>
<tr class="even">
<td>Punctuation removal</td>
<td>Word length filtering</td>
</tr>
</tbody>
</table>
</div>
<div id="references-2" class="section level2 unnumbered">
<h2>References</h2>
<p>Feinerer, I. (2008). An introduction to text mining in R. <em>R News</em>,
8(2), 19-22.</p>
<p>Feinerer, I., Hornik, K., &amp; Meyer, D. (2008). Text mining infrastructure
in R. <em>Journal of Statistical Software</em>, 25(5), 1-54.</p>
<p>Grün, B., &amp; Hornik, K. (2011). topicmodels: An R package for fitting
topic models. <em>Journal of Statistical Software</em>, 40(13), 1-30.</p>
<p>Ingersoll, G., Morton, T., &amp; Farris, L. (2012). <em>Taming Text: How to
find, organize and manipulate it</em>. Greenwich, CT: Manning Publications.</p>
</div>
<div id="exercises-16" class="section level2 unnumbered">
<h2>Exercises</h2>
<ol style="list-style-type: decimal">
<li><p>Take the recent annual reports for UPS<a href="#fn63" class="footnote-ref" id="fnref63"><sup>63</sup></a> and convert
them to text using an online service, such as
<a href="http://convertonlinefree.com/PDFToTXTEN.aspx"><u>http://www.fileformat.info/convert/doc/pdf2txt.htm</u></a>.
Complete the following tasks:</p>
<ol style="list-style-type: decimal">
<li><p>Count the words in the most recent annual report.</p></li>
<li><p>Compute the readability of the most recent annual report.</p></li>
<li><p>Create a corpus.</p></li>
<li><p>Preprocess the corpus.</p></li>
<li><p>Create a term-document matrix and compute the frequency of words
in the corpus.</p></li>
<li><p>Construct a word cloud for the 25 most common words.</p></li>
<li><p>Undertake a cluster analysis, identify which reports are similar
in nature, and see if you can explain why some reports are in
different clusters.</p></li>
<li><p>Build a topic model for the annual reports.</p></li>
</ol></li>
<li><p>Merge the annual reports for Berkshire Hathaway (i.e., Buffett’s
letters) and UPS into a single corpus.</p>
<ol style="list-style-type: decimal">
<li>Undertake a cluster analysis and identify which reports are
similar in nature.</li>
<li>Build a topic model for the combined annual reports.</li>
<li>Do the cluster analysis and topic model suggest considerable
differences in the two sets of reports?</li>
</ol></li>
</ol>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="56">
<li id="fn56"><p>Ambiguities are often the inspiration for puns. “You
can tune a guitar, but you can’t tuna fish. Unless of course, you
play bass,” by Douglas Adams<a href="text-mining-natural-language-processing.html#fnref56" class="footnote-back">↩︎</a></p></li>
<li id="fn57"><p><a href="http://www.berkshirehathaway.com/letters/letters.html" class="uri">http://www.berkshirehathaway.com/letters/letters.html</a><a href="text-mining-natural-language-processing.html#fnref57" class="footnote-back">↩︎</a></p></li>
<li id="fn58"><p>The converted letters are available at
<a href="http://www.terry.uga.edu/people/rwatson/">&lt;http://www.richardtwatson.com/BuffettLetters/&gt;</a>
and will be extended to include earlier and more recent letters. The
folder also contains the original letter in pdf.<a href="text-mining-natural-language-processing.html#fnref58" class="footnote-back">↩︎</a></p></li>
<li id="fn59"><p><a href="http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-smart-stop-list/english.stop" class="uri">http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-smart-stop-list/english.stop</a><a href="text-mining-natural-language-processing.html#fnref59" class="footnote-back">↩︎</a></p></li>
<li id="fn60"><p>The mathematics of LDA and CTM are beyond the scope of
this text. For details, see Grün, B., &amp; Hornik, K. (2011).
Topicmodels: An R package for fitting topic models. <em>Journal of
Statistical Software</em>, 40(13), 1-30. <a href="text-mining-natural-language-processing.html#fnref60" class="footnote-back">↩︎</a></p></li>
<li id="fn61"><p>Humans have a capacity to handle about 7±2 concepts at
a time. Miller, G. A. (1956). The magical number seven, plus or
minus two: some limits on our capacity for processing information.
<em>The Psychological Review</em>, 63(2), 81-97. <a href="text-mining-natural-language-processing.html#fnref61" class="footnote-back">↩︎</a></p></li>
<li id="fn62"><p><a href="http://opennlp.apache.org" class="uri">http://opennlp.apache.org</a><a href="text-mining-natural-language-processing.html#fnref62" class="footnote-back">↩︎</a></p></li>
<li id="fn63"><p><a href="http://www.investors.ups.com/phoenix.zhtml?c=62900&amp;p=irol-reportsannual" class="uri">http://www.investors.ups.com/phoenix.zhtml?c=62900&amp;p=irol-reportsannual</a><a href="text-mining-natural-language-processing.html#fnref63" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data-visualization-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="cluster-computing.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DataManagement.pdf", "DataManagement.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
